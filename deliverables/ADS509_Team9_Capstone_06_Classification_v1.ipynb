{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e49e21-4b1f-4603-a619-84b9e1998f03",
   "metadata": {},
   "source": [
    "# 599 Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67145b5b-5bfa-422d-9b5f-0b5ff807e3d3",
   "metadata": {},
   "source": [
    "The notebook is for text data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c57862-dbc2-4515-b62e-14adccbf6d2b",
   "metadata": {},
   "source": [
    "## Globally import libraries and set display parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34560828-2bee-4fc3-8880-41a6d291b1d6",
   "metadata": {},
   "source": [
    "Libraries needed mostly pertain to dataframe manipulation for data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c1e3ef-8619-4237-88c3-e4a06edaf6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import datetime as dt\n",
    "import emoji\n",
    "from icecream import ic\n",
    "from IPython.display import display_html \n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import regex as rex\n",
    "import shutil\n",
    "from string import punctuation\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, \\\n",
    "CountVectorizer, TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report, \\\n",
    "confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "import textacy.preprocessing as tprep\n",
    "from textacy.extract import keyword_in_context\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad016c12-a5ed-4e00-b8d4-068a03bb71dd",
   "metadata": {},
   "source": [
    "Set global parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84851602-fd72-48b8-913f-4f936ab7861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_start_time = time.perf_counter()\n",
    "\n",
    "random_state = 1699\n",
    "random.seed(random_state)\n",
    "\n",
    "# Set pandas global options\n",
    "pd.options.display.max_rows = 23\n",
    "pd.options.display.precision = 4\n",
    "np.set_printoptions(suppress=True,\n",
    "                    precision=4)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set tqdm package progress bar\n",
    "tqdm.pandas(ncols=50)\n",
    "\n",
    "dply_rng_end01 = 0\n",
    "dply_rng_end02 = 2\n",
    "\n",
    "to_csv_flag = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1823454-8427-4340-8a71-dd73fdf9932f",
   "metadata": {},
   "source": [
    "## Upload data from CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e499a4-03d2-4d9a-b83e-fe7a24840641",
   "metadata": {},
   "source": [
    "Establish working directories for saving dataframes as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e909d4-6cad-425c-8cee-2f0bfd303bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| curr_dir: 'C:\\\\Users\\\\acarr\\\\Documents\\\\GitHub\\\\599_team_project\\\\deliverables'\n",
      "ic| up1_dir: 'C:\\\\Users\\\\acarr\\\\Documents\\\\GitHub\\\\599_team_project'\n",
      "ic| 652109981.py:9 in <module> at 13:51:36.386\n"
     ]
    }
   ],
   "source": [
    "'''Dir nav citation:\n",
    "https://softhints.com/python-change-directory-parent/\n",
    "'''\n",
    "curr_dir = os.path.abspath(os.curdir)\n",
    "ic(curr_dir)\n",
    "os.chdir(\"..\")\n",
    "up1_dir = os.path.abspath(os.curdir)\n",
    "ic(up1_dir)\n",
    "ic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a019665-ecbd-4049-ba60-1bec1fc598b4",
   "metadata": {},
   "source": [
    "Get current date/time to append to file name string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5470e4-6f28-4d80-9c81-dac5293f235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| today: '2023-07-27_13-51-36424666'\n",
      "ic| type(today): <class 'str'>\n",
      "ic| 3990198718.py:6 in <module> at 13:51:36.496\n"
     ]
    }
   ],
   "source": [
    "today = dt.datetime.today()\n",
    "today= str(today)\n",
    "today = today.replace(':', '-').replace('.', '').replace(' ', '_')\n",
    "ic(today)\n",
    "ic(type(today))\n",
    "ic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867767d3-7156-4af0-9050-bda3165d5fca",
   "metadata": {},
   "source": [
    "Establish full file name path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f949b7-2dbb-4644-a792-35b2edf6c5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file in 1 path: C:\\Users\\acarr\\Documents\\GitHub\\599_team_project\\data_large\\capstone_master_tm_X01_v1.csv\n",
      "NP array file in 2 path: C:\\Users\\acarr\\Documents\\GitHub\\599_team_project\\data\\capstone_master_tm_y01_v1.npy\n"
     ]
    }
   ],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_large_location = 'data_large'\n",
    "data_location = 'data'\n",
    "ref_docs_location = 'ref_docs'\n",
    "\n",
    "file_in_name01 = 'capstone_master_tm_X01_v1.csv'\n",
    "file_in_name02 = 'capstone_master_tm_y01_v1.npy'\n",
    "\n",
    "file_in_path01 = os.path.join(up1_dir, data_large_location, file_in_name01)\n",
    "file_in_path02 = os.path.join(up1_dir, data_location, file_in_name02)\n",
    "\n",
    "print(f'CSV file in 1 path: {file_in_path01}')\n",
    "print(f'NP array file in 2 path: {file_in_path02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad30d7f-1009-4e26-b659-068edb48d048",
   "metadata": {},
   "source": [
    "### Review dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92c4626-d457-49ec-9d1e-2cd64dc4c702",
   "metadata": {},
   "source": [
    "Read in data from CSV, check resulting dataframe shape, and display first several records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "001ea078-0f38-4692-b6bb-318957f1cb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (36402, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>ever wanted keg titos handmade vodka dream bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>five months julian sands went missing solo hik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>four star running back picks michigan state un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>alabama center charles bediako signs one year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>ralph sampson breaks iconic boston houston roc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  text_id source_name  \\\n",
       "0           0        2   USA Today   \n",
       "1           1        3   USA Today   \n",
       "2           2        5   USA Today   \n",
       "3           3        6   USA Today   \n",
       "4           4        7   USA Today   \n",
       "\n",
       "                                      processed_text  \n",
       "0  ever wanted keg titos handmade vodka dream bec...  \n",
       "1  five months julian sands went missing solo hik...  \n",
       "2  four star running back picks michigan state un...  \n",
       "3  alabama center charles bediako signs one year ...  \n",
       "4  ralph sampson breaks iconic boston houston roc...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X01_df01 = pd.read_csv(file_in_path01)\n",
    "print(f'Dataframe shape: {X01_df01.shape}')\n",
    "display(X01_df01.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c500ea0-250a-46fc-aea6-740efd917e35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP array shape: (36402, 10)\n",
      "[[0 0 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "y01_arr01 = np.load(file_in_path02)\n",
    "print(f'NP array shape: {y01_arr01.shape}')\n",
    "print(y01_arr01[:23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d78dfc79-a253-423d-90fe-2c0540dd2959",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X01_df01['multilabel'] = y01_arr01.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "017ee611-4e56-4ea2-ae0b-1353d51e77fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (36402, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>multilabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>ever wanted keg titos handmade vodka dream bec...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>five months julian sands went missing solo hik...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>four star running back picks michigan state un...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>alabama center charles bediako signs one year ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>ralph sampson breaks iconic boston houston roc...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  text_id source_name  \\\n",
       "0           0        2   USA Today   \n",
       "1           1        3   USA Today   \n",
       "2           2        5   USA Today   \n",
       "3           3        6   USA Today   \n",
       "4           4        7   USA Today   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  ever wanted keg titos handmade vodka dream bec...   \n",
       "1  five months julian sands went missing solo hik...   \n",
       "2  four star running back picks michigan state un...   \n",
       "3  alabama center charles bediako signs one year ...   \n",
       "4  ralph sampson breaks iconic boston houston roc...   \n",
       "\n",
       "                       multilabel  \n",
       "0  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 1, 0, 1]  \n",
       "2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Dataframe shape: {X01_df01.shape}')\n",
    "display(X01_df01.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac4fee5c-dd14-4381-a686-84b01e05f91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29121,)\n",
      "(29121, 10)\n",
      "\n",
      "(7281,)\n",
      "(7281, 10)\n",
      "15749    guess whos back former nfl coach john perry fo...\n",
      "32908    forbes worlds richest people become b wealthie...\n",
      "2433     experts answer readers home buying questions w...\n",
      "2470     missed boat kellen moore recalls bad break qb ...\n",
      "12351    london prosecutor calls oscar winning actor ke...\n",
      "                               ...                        \n",
      "22854    month third incarnation xfl concluded season l...\n",
      "32711    rangers rival rangers islanders goalies push c...\n",
      "35423    connecticut state representative assaulted att...\n",
      "32846    evp digital energy global business schneider e...\n",
      "19609    echo employees three years flexibility hybrid ...\n",
      "Name: processed_text, Length: 29121, dtype: object\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "#X_train, y_train, X_test, y_test = iterative_train_test_split(x, y, test_size = 0.1)\n",
    "\n",
    "nlm_train_x01, \\\n",
    "nlm_test_x01, \\\n",
    "nlm_train_y01, \\\n",
    "nlm_test_y01 = train_test_split(X01_df01['processed_text'],\n",
    "                                y01_arr01,\n",
    "                                test_size=.2,\n",
    "                                stratify=y01_arr01,\n",
    "                                random_state=random_state)\n",
    "\n",
    "#nlm_train_y01 = nlm_train_y01.ravel()\n",
    "#nlm_test_y01 = nlm_test_y01.ravel()\n",
    "\n",
    "print(f'{nlm_train_x01.shape}')\n",
    "print(f'{nlm_train_y01.shape}')\n",
    "print(f'\\n{nlm_test_x01.shape}')\n",
    "print(f'{nlm_test_y01.shape}')\n",
    "\n",
    "print(nlm_train_x01)\n",
    "print(nlm_train_y01)\n",
    "print(type(nlm_train_y01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b221e7f-309e-44d7-b91f-a5f8ab3564d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN values in nlm_train_y01.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming nlm_train_y01 is already defined as a NumPy array\n",
    "\n",
    "# Check for NaN values in nlm_train_y01\n",
    "has_nan_values = np.isnan(nlm_train_y01).any()\n",
    "\n",
    "if has_nan_values:\n",
    "    print(\"NaN values exist in nlm_train_y01.\")\n",
    "else:\n",
    "    print(\"No NaN values in nlm_train_y01.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b28673-bb01-4d62-8b42-9ee7b3ddc6da",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3767b75f-5667-4a66-be84-0a03e2f77b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36402,)\n",
      "0    ever wanted keg titos handmade vodka dream bec...\n",
      "1    five months julian sands went missing solo hik...\n",
      "2    four star running back picks michigan state un...\n",
      "3    alabama center charles bediako signs one year ...\n",
      "4    ralph sampson breaks iconic boston houston roc...\n",
      "Name: processed_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X01_df01['processed_text'].shape)\n",
    "print(X01_df01['processed_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3d91cfa-e297-415f-9fa1-21c1e7df827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '', '️', 'arent', 'cannot', 'cant', 'couldnt', 'couldve', 'didnt', 'doesnt', 'dont', 'hadnt', 'hasnt', 'havent', 'hes', 'im', \"i'm\", 'isnt', 'it’s', 'ive', '𝚘𝚏', 'mightnt', 'mustnt', 'neednt', 'shant', 'shes', 'shouldnt', 'shouldve', 'thatll', 'theyll', 'theyve', 'wasnt', 'werent', 'whats', 'weve', 'wont', 'wouldnt', 'wouldve', 'yall', 'youd', 'youll', 'youre', 'youve', \"we'll\", 'you’re', 'you’ve', 'you’ll', 'you’d', 'she’s', 'it’s', 'that’ll', 'don’t', 'should’ve', 'aren’t', 'couldn’t', 'didn’t', 'doesn’t', 'hadn’t', 'hasn’t', 'haven’t', 'isn’t', 'mightn’t', 'mustn’t', 'needn’t', 'shan’t', 'shouldn’t', 'wasn’t', 'weren’t', 'won’t', 'wouldn’t', 'i’m', 'we’ll', 'said', 'told', 'according', 'reporting', 'reported', 'statement', 'spoke', 'next', 'though', 'often', 'story', 'updated', 'additional', 'developments', 'follow', 'published', 'com', 'sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', 'via', 'account', 'accounts', 'article', 'advertisement', 'advertisements']\n"
     ]
    }
   ],
   "source": [
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# Customize nltk stop word list\n",
    "sw.extend(['',\n",
    "           '️',\n",
    "           'arent',\n",
    "           'cannot',\n",
    "           'cant',\n",
    "           'couldnt',\n",
    "           'couldve',\n",
    "           'didnt',\n",
    "           'doesnt',\n",
    "           'dont',\n",
    "           'hadnt',\n",
    "           'hasnt',\n",
    "           'havent',\n",
    "           'hes',\n",
    "           'im',\n",
    "           \"i'm\",\n",
    "           'isnt',\n",
    "           'it’s',\n",
    "           'ive',\n",
    "           '𝚘𝚏',\n",
    "           'mightnt',\n",
    "           'mustnt',\n",
    "           'neednt',\n",
    "           'shant',\n",
    "           'shes',\n",
    "           'shouldnt',\n",
    "           'shouldve',\n",
    "           'thatll',\n",
    "           'theyll',\n",
    "           'theyve',\n",
    "           'wasnt',\n",
    "           'werent',\n",
    "           'whats',\n",
    "           'weve',\n",
    "           'wont',\n",
    "           'wouldnt',\n",
    "           'wouldve',\n",
    "           'yall',\n",
    "           'youd',\n",
    "           'youll',\n",
    "           'youre',\n",
    "           'youve',\n",
    "           \"we'll\",\n",
    "           \"you’re\",\n",
    "           \"you’ve\",\n",
    "           \"you’ll\",\n",
    "           \"you’d\",\n",
    "           \"she’s\",\n",
    "           \"it’s\",\n",
    "           \"that’ll\",\n",
    "           \"don’t\",\n",
    "           \"should’ve\",\n",
    "           \"aren’t\",\n",
    "           \"couldn’t\",\n",
    "           \"didn’t\",\n",
    "           \"doesn’t\",\n",
    "           \"hadn’t\",\n",
    "           \"hasn’t\",\n",
    "           \"haven’t\",\n",
    "           \"isn’t\",\n",
    "           \"mightn’t\",\n",
    "           \"mustn’t\",\n",
    "           \"needn’t\",\n",
    "           \"shan’t\",\n",
    "           \"shouldn’t\",\n",
    "           \"wasn’t\",\n",
    "           \"weren’t\",\n",
    "           \"won’t\",\n",
    "           \"wouldn’t\",\n",
    "           \"i’m\",\n",
    "           \"we’ll\",\n",
    "           'said',\n",
    "           'told',\n",
    "           'according',\n",
    "           'reporting',\n",
    "           'reported',\n",
    "           'statement',\n",
    "           'spoke',\n",
    "           'next',\n",
    "           'though',\n",
    "           'often',\n",
    "           'story',\n",
    "           'updated',\n",
    "           'additional',\n",
    "           'developments',\n",
    "           'follow',\n",
    "           'published',\n",
    "           'com',\n",
    "           'sunday',\n",
    "           'monday',\n",
    "           'tuesday',\n",
    "           'wednesday',\n",
    "           'thursday',\n",
    "           'friday',\n",
    "           'saturday',\n",
    "           'january',\n",
    "           'february',\n",
    "           'march',\n",
    "           'april',\n",
    "           'may',\n",
    "           'june',\n",
    "           'july',\n",
    "           'august',\n",
    "           'september',\n",
    "           'october',\n",
    "           'november',\n",
    "           'december',\n",
    "           'via',\n",
    "           'account',\n",
    "           'accounts',\n",
    "           'article',\n",
    "           'advertisement',\n",
    "           'advertisements',\n",
    "          ])\n",
    "\n",
    "print(len(sw))\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "008b1190-a0f0-4797-9bc9-fa5b9a1715dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<29121x398234 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12196755 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<7281x398234 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2956083 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed processing time = 0.7723571333333333 mins\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "nlm_tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                            analyzer='word',\n",
    "                            stop_words=sw,\n",
    "                            token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "                            ngram_range=(1,2),\n",
    "                            max_df=.7,\n",
    "                            min_df=5)\n",
    "\n",
    "nlm_train_x01_mtx = nlm_tfidf.fit_transform(nlm_train_x01)\n",
    "nlm_test_x01_mtx = nlm_tfidf.transform(nlm_test_x01)\n",
    "\n",
    "display(nlm_train_x01_mtx)\n",
    "display(nlm_test_x01_mtx)\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print(f'\\nElapsed processing time = {(end_time - start_time) / 60} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb01aa7c-d5b7-4d9c-951c-30a73645edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_samp_dwm(sm=None,\n",
    "                     vec=None,\n",
    "                     n=(1,1),\n",
    "                     rs_tup=(1,1)):\n",
    "    mtx_df01 = pd.DataFrame(sm.toarray(),\n",
    "                            columns=vec.get_feature_names_out())\n",
    "\n",
    "    mtx_df01a = mtx_df01.sample(n=n[0],\n",
    "                                random_state=rs_tup[0],\n",
    "                                axis=1)\n",
    "\n",
    "    mtx_df01b = mtx_df01a.sample(n=n[1],\n",
    "                                 random_state=rs_tup[1],\n",
    "                                 axis=0)\n",
    "\n",
    "    display(mtx_df01b)\n",
    "    return vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b95e98f8-ce69-4f7f-bba3-22636f0067ea",
   "metadata": {},
   "source": [
    "rs_tup = (random_state,random_state)\n",
    "n = (10,10)\n",
    "nlm_train_x01_mtx_cols = display_samp_dwm(sm=nlm_train_x01_mtx,\n",
    "                                          vec=nlm_tfidf,\n",
    "                                          n=n,\n",
    "                                          rs_tup=rs_tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a5f4b7-0598-4910-908b-9964b08d2640",
   "metadata": {},
   "source": [
    "### Write file without stop words to CSV - data subset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190c05e6-32cc-4588-843b-319ff7a26941",
   "metadata": {},
   "source": [
    "Set path to write CSV file to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "360e6ee9-271f-4125-91f5-5303496eed00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file out 1 path: C:\\Users\\acarr\\Documents\\GitHub\\599_team_project\\data\\data_preprocessed_wo_sw_X_half1_2023-07-27_13-51-36424666.csv\n",
      "CSV file out 2 path: C:\\Users\\acarr\\Documents\\GitHub\\599_team_project\\data\\data_preprocessed_wo_sw_X_half2_2023-07-27_13-51-36424666.csv\n"
     ]
    }
   ],
   "source": [
    "file_out_name01 = f'data_preprocessed_wo_sw_X_half1_{today}.csv'\n",
    "file_out_name02 = f'data_preprocessed_wo_sw_X_half2_{today}.csv'\n",
    "\n",
    "file_out_path01 = os.path.join(up1_dir, data_location, file_out_name01)\n",
    "file_out_path02 = os.path.join(up1_dir, data_location, file_out_name02)\n",
    "\n",
    "print(f'CSV file out 1 path: {file_out_path01}')\n",
    "print(f'CSV file out 2 path: {file_out_path02}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21f19a19-da86-431b-bbb2-70037dd8f8ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "export_col_names_lst = ['text_id', 'source_name', 'processed_text', 'multilabel']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd93d8-2a30-4d88-8f0e-ac544b21210b",
   "metadata": {},
   "source": [
    "Write pandas dataframe to CSV; save locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "847166c2-4068-4ba9-b09a-f61d520d042d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18201\n"
     ]
    }
   ],
   "source": [
    "len_half = int(round(len(X01_df01)/2,0))\n",
    "print(len_half)\n",
    "\n",
    "if to_csv_flag == False:\n",
    "    pass\n",
    "else:\n",
    "    X01_df01[export_col_names_lst][:len_half].to_csv(file_out_path01,\n",
    "                                                               index=False)\n",
    "    X01_df01[export_col_names_lst][len_half:].to_csv(file_out_path02,\n",
    "                                                               index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "253d3912-5010-443c-a715-b868ce61fe6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36402, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X01_df01.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ef80f4-ded6-482b-a6e8-8045ca1fe1ea",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c927c-368d-41fb-989b-856453c86d2a",
   "metadata": {},
   "source": [
    "### Algorithm setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95916dce-fb7f-4531-842e-288a965bef58",
   "metadata": {},
   "source": [
    "### Random Forests Classifier - Using `BayesSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d56fd-08d5-4168-b1bd-3e55110e6252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Multiclass and multioutput algortihm citation:\n",
    "https://scikit-learn.org/stable/modules\n",
    "/multiclass.html#multiclass-multioutput-classification\n",
    "'''\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "nlm_train_x01_mtx\n",
    "nlm_train_y01\n",
    "#X, y1 = make_classification(n_samples=10, n_features=100,\n",
    "#y2 = shuffle(y1, random_state=1)\n",
    "#y3 = shuffle(y1, random_state=2)\n",
    "#Y = np.vstack((y1, y2, y3)).T\n",
    "start_time = time.perf_counter()\n",
    "n_samples, n_features = nlm_train_x01_mtx.shape # 10,100\n",
    "n_outputs = nlm_train_y01.shape[1] # 3\n",
    "n_classes = 10\n",
    "forest = RandomForestClassifier(random_state=1)\n",
    "multi_target_forest = MultiOutputClassifier(forest)\n",
    "multi_target_forest.fit(nlm_train_x01_mtx, nlm_train_y01)\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print(f'\\nElapsed processing time = {(end_time - start_time) / 60} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71065b3c-c9d3-4940-8e25-73637b327612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#writable_nlm_train_x01_mtx = np.copy(nlm_train_x01_mtx)\n",
    "\n",
    "multi_target_forest.predict(nlm_train_x01_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9bf7a8-2eb1-45c6-af4e-c3526e9b40e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start timer script\n",
    "start_time = dt.datetime.today()\n",
    "\n",
    "# Citation: Hochberg, 2018; Shanmukh, 2021\n",
    "m1v1_rfc_pip = Pipeline([('rfc',\n",
    "                          RandomForestClassifier(random_state=random_state))])\n",
    "\n",
    "rskf_splits = 2\n",
    "rskf_repeats = 2\n",
    "bsg_iters = 5\n",
    "bsg_jobs = 2\n",
    "\n",
    "#loss_hparam = Categorical(['log_loss', 'exponential'])\n",
    "#lrate_hparam = Real(1e-3, 1e3, prior='log-uniform')\n",
    "#nest_hparam = Integer(1e2, 1e3, prior='log-uniform')\n",
    "\n",
    "nest_hparam = Integer(1e2, 1e3, prior='log-uniform')\n",
    "mndepth_hparam = Integer(1e0, 1e2, prior='log-uniform')\n",
    "mnsamps_hparam = Integer(1e0, 1e2, prior='log-uniform')\n",
    "mnsampl_hparam = Integer(1e0, 1e2, prior='log-uniform')\n",
    "mxfeat_hparam = Categorical(['sqrt', 'log2', None])\n",
    "minimpd_hparam = Real(1e-3, 1e3, prior='log-uniform')\n",
    "oob_hparam = Categorical([True, False])\n",
    "ccp_hparam = Real(1e-3, 1e3, prior='log-uniform')\n",
    "maxsamp_hparam = Real(1e-3, 1e0, prior='log-uniform')\n",
    "\n",
    "m1v1_rfc_grd = {'rfc__n_estimators': nest_hparam,\n",
    "                'rfc__max_depth': mndepth_hparam,\n",
    "                'rfc__min_samples_split': mnsamps_hparam,\n",
    "                'rfc__min_samples_leaf': mnsampl_hparam,\n",
    "                'rfc__max_features': mxfeat_hparam,\n",
    "                'rfc__min_impurity_decrease': minimpd_hparam,\n",
    "                'rfc__oob_score': oob_hparam,\n",
    "                'rfc__ccp_alpha': ccp_hparam,\n",
    "                'rfc__max_samples': maxsamp_hparam,\n",
    "               }\n",
    "\n",
    "'''Change rfc default scoring from accuracy to F1 score citation:\n",
    "https://chat.openai.com/share/254f382b-4a8e-48e8-acd5-2918f0bbc59d\n",
    "'''\n",
    "f1_scorer = make_scorer(f1_score,\n",
    "                        pos_label='right')\n",
    "\n",
    "'''Customize cross-validation citation:\n",
    "https://machinelearningmastery.com\n",
    "/scikit-optimize-for-hyperparameter-tuning-in-machine-learning/\n",
    "'''\n",
    "cv = RepeatedStratifiedKFold(n_splits=rskf_splits,\n",
    "                             n_repeats=rskf_repeats,\n",
    "                             random_state=random_state)\n",
    "\n",
    "m1v1_rfc = BayesSearchCV(m1v1_rfc_pip,\n",
    "                         m1v1_rfc_grd,\n",
    "                         n_iter=bsg_iters,\n",
    "                         scoring=None,\n",
    "                         cv=cv,\n",
    "                         n_jobs=bsg_jobs,\n",
    "                         refit=True,\n",
    "                         verbose=4,\n",
    "                         random_state=random_state)\n",
    "\n",
    "m1v1_rfc.fit(nlm_train_x01_mtx, nlm_train_y01)\n",
    "\n",
    "# End timer script\n",
    "end_time = dt.datetime.today()\n",
    "time_elapse = end_time - start_time\n",
    "print(f'Start Time = {start_time}')\n",
    "print(f'End Time = {end_time}')\n",
    "print(f'Elapsed Time = {time_elapse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6dd32-ad15-41e9-bdc3-d7f14c6b134b",
   "metadata": {},
   "source": [
    "### Pickle best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf00d7-8a7e-4d8d-ad8d-3b31e3c96dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the pickled model\n",
    "mod_folder_name = 'trained_models'\n",
    "m1v1_pkl_file_name = 'm1v1_rfc.pkl'\n",
    "\n",
    "pkl_file_path01 = os.path.join(curr_dir, mod_folder_name, m1v1_pkl_file_name)\n",
    "\n",
    "print(f'Pickle file 1 in path: {pkl_file_path01}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c88de250-0ddd-412f-b788-6c209cc9197c",
   "metadata": {},
   "source": [
    "with open(pkl_file_path01, \"wb\") as file:\n",
    "    pickle.dump(m1v1_rfc, file)\n",
    "\n",
    "print(\"Model pickled and saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e5e32b-9213-4fff-8507-cc24c31c2a4c",
   "metadata": {},
   "source": [
    "### Load pickled best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f534eb-0da9-44ad-bab0-0e8ceccf6b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file_path01, 'rb') as file:\n",
    "    m1v1_rfc = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13940394-3455-4f0e-bf4d-f9f7b29fc96a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'\\nBest Estimator:\\n{m1v1_rfc.best_estimator_}')\n",
    "\n",
    "print('\\nCross-validaton results:')\n",
    "display(pd.DataFrame(m1v1_rfc.cv_results_))\n",
    "\n",
    "train_m1v1_rfc_y01_pred = m1v1_rfc.predict_proba(nlm_train_x01_mtx)\n",
    "print(f'\\nFirst 10 train set predictions:\\n{train_m1v1_rfc_y01_pred[:10]}')\n",
    "\n",
    "test_m1v1_rfc_y01_pred = m1v1_rfc.predict_proba(nlm_test_x01_mtx)\n",
    "print(f'\\nFirst 10 test set predictions:\\n{test_m1v1_rfc_y01_pred[:10]}')\n",
    "\n",
    "print(f'\\nBest Score for \"{m1v1_rfc.scorer_}\" is {m1v1_rfc.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a55d3e6-439a-42fe-810a-793383d4a049",
   "metadata": {},
   "source": [
    "#### Train set check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d511ab6-9831-414c-b46e-12e44b3ab6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_train_y01_pred = m1v1_rfc.predict(nlm_train_x01_mtx)\n",
    "nlm_train_y01_pred_cm = confusion_matrix(nlm_train_y01, nlm_train_y01_pred)\n",
    "\n",
    "print(classification_report(nlm_train_y01, nlm_train_y01_pred))\n",
    "print(nlm_train_y01_pred_cm)\n",
    "\n",
    "'''Citation:\n",
    "https://scikit-learn.org/stable/modules/generated\n",
    "/sklearn.metrics.ConfusionMatrixDisplay.html\n",
    "#sklearn.metrics.ConfusionMatrixDisplay.plot\n",
    "'''\n",
    "nlm_train_cm_dsp = ConfusionMatrixDisplay(confusion_matrix=nlm_train_y01_pred_cm,\n",
    "                                          display_labels=m1v1_rfc.classes_)\n",
    "nlm_train_cm_dsp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62e4669-850e-4314-b2f2-8b1cb962d49c",
   "metadata": {},
   "source": [
    "#### ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1cdd67-f8cf-44fb-b93a-8e844c1599fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_train_y01_pred_decf = m1v1_rfc.decision_function(nlm_train_x01_mtx)\n",
    "RocCurveDisplay.from_predictions(nlm_train_y01, nlm_train_y01_pred_decf,\n",
    "                                 pos_label='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a8fdc-dfc2-4797-9540-13c4c83c920b",
   "metadata": {},
   "source": [
    "#### Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e26cf8-90e6-450c-a84c-7858d9a653c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_test_y01_pred = m1v1_rfc.predict(nlm_test_x01_mtx)\n",
    "nlm_test_y01_pred_cm = confusion_matrix(nlm_test_y01, nlm_test_y01_pred)\n",
    "\n",
    "print('Test Set Evaluation Metrics')\n",
    "print(classification_report(nlm_test_y01, nlm_test_y01_pred))\n",
    "print(nlm_test_y01_pred_cm)\n",
    "\n",
    "'''Citation:\n",
    "https://scikit-learn.org/stable/modules/generated\n",
    "/sklearn.metrics.ConfusionMatrixDisplay.html\n",
    "#sklearn.metrics.ConfusionMatrixDisplay.plot\n",
    "'''\n",
    "nlm_test_cm_dsp = ConfusionMatrixDisplay(confusion_matrix=nlm_test_y01_pred_cm,\n",
    "                                         display_labels=m1v1_rfc.classes_)\n",
    "nlm_test_cm_dsp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22fe472-ae95-4343-9924-3a8677b180e0",
   "metadata": {},
   "source": [
    "#### Variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba611e-a789-4cb0-a7a3-83da62fd0bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlm_train_x01_mtx_cols)\n",
    "print(type(nlm_train_x01_mtx_cols))\n",
    "print(nlm_train_x01_mtx_cols.shape)\n",
    "\n",
    "x = m1v1_rfc.best_estimator_.named_steps['rfc'].feature_importances_\n",
    "x_df01 = pd.DataFrame(x, columns=['var_imp'])\n",
    "x_df01['feature'] = nlm_train_x01_mtx_cols\n",
    "x_df02 = x_df01.sort_values(by=['var_imp'], ascending=False)\n",
    "x_df03 = x_df02.head(20)\n",
    "\n",
    "display(x_df02.head())\n",
    "print(type(x_df02))\n",
    "print(x_df02.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe0294-236e-4d5c-bba7-63edeb108d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Citation:\n",
    "https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
    "'''\n",
    "# plot feature importance\n",
    "#figure = plt.figsize((10,9))\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Feature Importance (Top 20)')\n",
    "plt.barh([x for x in range(len(x_df03['var_imp']))], x_df03['var_imp'],\n",
    "         tick_label=x_df03['feature'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f919899-b8a0-4c89-8519-abbf79184202",
   "metadata": {},
   "outputs": [],
   "source": [
    "TNmodel1=nlm_test_y01_pred_cm[0][0]\n",
    "FPmodel1=nlm_test_y01_pred_cm[0][1]\n",
    "FNmodel1=nlm_test_y01_pred_cm[1][0]\n",
    "TPmodel1=nlm_test_y01_pred_cm[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b9eb8-b9c4-41d8-9f06-8c5efdb1d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results:\n",
    "from tabulate import tabulate\n",
    "\n",
    "TANmodel1=TNmodel1+FPmodel1\n",
    "TAPmodel1=TPmodel1+FNmodel1\n",
    "TPPmodel1=FPmodel1+TPmodel1\n",
    "TPNmodel1=TNmodel1+FNmodel1\n",
    "GTmodel1=TANmodel1+TAPmodel1\n",
    "AccuracyM1=(TNmodel1+TPmodel1)/GTmodel1\n",
    "ErrorRateM1=1-AccuracyM1\n",
    "SensitivityM1=TPmodel1/(TAPmodel1)\n",
    "RecallM1=SensitivityM1\n",
    "SpecificityM1=TNmodel1/TANmodel1\n",
    "PrecisionM1=TPmodel1/TPPmodel1\n",
    "F1M1=2*PrecisionM1*RecallM1/(PrecisionM1 + RecallM1)\n",
    "F2M1=5*(PrecisionM1*RecallM1)/((4*PrecisionM1)+RecallM1)\n",
    "Fp5M1=(1.25)*(PrecisionM1*RecallM1)/((0.25*PrecisionM1)+RecallM1)\n",
    "\n",
    "header = [\"Accuracy\", \"Error Rate\", \"Sensitivity\", \"Recall\", \"Specificity\",\n",
    "          \"Precision\", \"F1\", \"F2\", \"F0.5\"]\n",
    "data1 = [[\"Accuracy\", AccuracyM1], [\"Error Rate\", ErrorRateM1],\n",
    "         [\"Sensitivity\", SensitivityM1],\n",
    "         [\"Recall\", RecallM1], [\"Specificity\", SpecificityM1],\n",
    "         [\"Precision\", PrecisionM1],\n",
    "         [\"F1\", F1M1], [\"F2\", F2M1], [\"F0.5\", Fp5M1]]\n",
    "\n",
    "col_names=[\"Measurement\", \"Linear SVC Model\"]\n",
    "\n",
    "ModelEvaluationTable = tabulate(data1, headers=col_names,\n",
    "                                tablefmt=\"fancy_grid\")\n",
    "\n",
    "print(ModelEvaluationTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c83aa-1045-4e18-b0b6-7a31dff22bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d66ce1-0dae-4c5f-92d4-53204a368966",
   "metadata": {},
   "source": [
    "Set list of dataframe columns to export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c22a70-0395-47a8-97cb-e13561cc0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_col_names_lst = ['processed_text',\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25cadab-f54e-489a-a695-d3d2672a80e6",
   "metadata": {},
   "source": [
    "## Display runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e50fc1-03b2-4be8-b29b-8ff56361b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_end_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f84127-4840-4f28-a604-c810ff0afca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\\nElapsed processing time = {round((global_end_time \n",
    "- global_start_time)/60,2)} mins''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b2704-5b1e-4a85-80d2-ed16426a8237",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
