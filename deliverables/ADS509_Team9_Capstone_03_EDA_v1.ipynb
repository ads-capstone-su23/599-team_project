{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e49e21-4b1f-4603-a619-84b9e1998f03",
   "metadata": {},
   "source": [
    "# 509 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67145b5b-5bfa-422d-9b5f-0b5ff807e3d3",
   "metadata": {},
   "source": [
    "The notebook is for Exploratory Data Analysis (EDA), text data preprocessing, modeling, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c57862-dbc2-4515-b62e-14adccbf6d2b",
   "metadata": {},
   "source": [
    "## Globally import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3c1e3ef-8619-4237-88c3-e4a06edaf6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import datetime as dt\n",
    "import emoji\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import regex as rex\n",
    "import requests\n",
    "import shutil\n",
    "from string import punctuation\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, \\\n",
    "CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report, \\\n",
    "confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "import textacy.preprocessing as tprep\n",
    "from textacy.extract import keyword_in_context\n",
    "\n",
    "# Set pandas global options\n",
    "pd.options.display.max_rows = 17\n",
    "pd.options.display.precision = 4\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2134bc38-3fb5-4804-9616-cc3f7f940201",
   "metadata": {},
   "source": [
    "! pip install bertopic[spacy]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "502a09bf-1efd-4c18-9f75-4764db30d141",
   "metadata": {},
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1823454-8427-4340-8a71-dd73fdf9932f",
   "metadata": {},
   "source": [
    "## Upload data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e909d4-6cad-425c-8cee-2f0bfd303bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acarr\\Documents\\GitHub\\599_team_project\\deliverables\n",
      "C:\\Users\\acarr\\Documents\\GitHub\\599_team_project\n"
     ]
    }
   ],
   "source": [
    "'''Dir nav citation:\n",
    "https://softhints.com/python-change-directory-parent/'''\n",
    "curr_dir = os.path.abspath(os.curdir)\n",
    "print(curr_dir)\n",
    "os.chdir(\"..\")\n",
    "up1_dir = os.path.abspath(os.curdir)\n",
    "print(up1_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f949b7-2dbb-4644-a792-35b2edf6c5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 1 in path: C:\\Users\\acarr\\Documents\\GitHub\\599_team_project\\data\\data_scraped_amc_2023-07-10_13-41-28670390.csv\n",
      "CSV file 2 in path: C:\\Users\\acarr\\Documents\\GitHub\\599_team_project\\data\\master_business_TheHill.csv\n"
     ]
    }
   ],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = 'data'\n",
    "\n",
    "file_in_name01 = 'data_scraped_amc_2023-07-10_13-41-28670390.csv'\n",
    "file_in_name02 = 'master_business_TheHill.csv'\n",
    "\n",
    "file_in_path01 = os.path.join(up1_dir, data_location, file_in_name01)\n",
    "file_in_path02 = os.path.join(up1_dir, data_location, file_in_name02)\n",
    "\n",
    "print(f'CSV file 1 in path: {file_in_path01}')\n",
    "print(f'CSV file 2 in path: {file_in_path02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad30d7f-1009-4e26-b659-068edb48d048",
   "metadata": {},
   "source": [
    "### Review dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "001ea078-0f38-4692-b6bb-318957f1cb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (36420, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>article_text</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>USA TODAY, Emily DeLetter, USA TODAY</td>\n",
       "      <td>Tito's launches 'Tito's in a Big Can,' an empt...</td>\n",
       "      <td>https://www.usatoday.com/story/money/food/2023...</td>\n",
       "      <td>2023-06-21T17:37:40Z</td>\n",
       "      <td>Have you ever wanted to own your very own keg ...</td>\n",
       "      <td>Have you ever wanted to own your very own keg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>USA TODAY, Joy Ashford, USA TODAY</td>\n",
       "      <td>Search for missing actor Julian Sands continue...</td>\n",
       "      <td>https://www.usatoday.com/story/entertainment/c...</td>\n",
       "      <td>2023-06-20T17:36:09Z</td>\n",
       "      <td>Over five months after Julian Sands went missi...</td>\n",
       "      <td>Over five months after Julian Sandswent missin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>Tar Heels Wire</td>\n",
       "      <td>Four star running back picks Michigan State ov...</td>\n",
       "      <td>https://tarheelswire.usatoday.com/2023/06/24/f...</td>\n",
       "      <td>2023-06-25T03:51:10Z</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFour star running ...</td>\n",
       "      <td>Mack Brown and the UNC football program have b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>Roll Tide Wire</td>\n",
       "      <td>Alabama center Charles Bediako signs one-year ...</td>\n",
       "      <td>https://rolltidewire.usatoday.com/2023/06/23/a...</td>\n",
       "      <td>2023-06-23T21:29:24Z</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAlabama center Cha...</td>\n",
       "      <td>Alabama center Charles Bediako was signed to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>USA Today</td>\n",
       "      <td>Celtics Wire</td>\n",
       "      <td>Ralph Sampson breaks down iconic Boston Celtic...</td>\n",
       "      <td>https://celticswire.usatoday.com/2023/06/23/nb...</td>\n",
       "      <td>2023-06-23T11:00:41Z</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRalph Sampson brea...</td>\n",
       "      <td>It was one of the most memorable moments in NB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id source_name                                author  \\\n",
       "0        2   USA Today  USA TODAY, Emily DeLetter, USA TODAY   \n",
       "1        3   USA Today     USA TODAY, Joy Ashford, USA TODAY   \n",
       "2        5   USA Today                        Tar Heels Wire   \n",
       "3        6   USA Today                        Roll Tide Wire   \n",
       "4        7   USA Today                          Celtics Wire   \n",
       "\n",
       "                                               title  \\\n",
       "0  Tito's launches 'Tito's in a Big Can,' an empt...   \n",
       "1  Search for missing actor Julian Sands continue...   \n",
       "2  Four star running back picks Michigan State ov...   \n",
       "3  Alabama center Charles Bediako signs one-year ...   \n",
       "4  Ralph Sampson breaks down iconic Boston Celtic...   \n",
       "\n",
       "                                                 url          publish_date  \\\n",
       "0  https://www.usatoday.com/story/money/food/2023...  2023-06-21T17:37:40Z   \n",
       "1  https://www.usatoday.com/story/entertainment/c...  2023-06-20T17:36:09Z   \n",
       "2  https://tarheelswire.usatoday.com/2023/06/24/f...  2023-06-25T03:51:10Z   \n",
       "3  https://rolltidewire.usatoday.com/2023/06/23/a...  2023-06-23T21:29:24Z   \n",
       "4  https://celticswire.usatoday.com/2023/06/23/nb...  2023-06-23T11:00:41Z   \n",
       "\n",
       "                                        article_text  \\\n",
       "0  Have you ever wanted to own your very own keg ...   \n",
       "1  Over five months after Julian Sands went missi...   \n",
       "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFour star running ...   \n",
       "3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAlabama center Cha...   \n",
       "4  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRalph Sampson brea...   \n",
       "\n",
       "                                             content  \n",
       "0  Have you ever wanted to own your very own keg ...  \n",
       "1  Over five months after Julian Sandswent missin...  \n",
       "2  Mack Brown and the UNC football program have b...  \n",
       "3  Alabama center Charles Bediako was signed to a...  \n",
       "4  It was one of the most memorable moments in NB...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "slct_tbl_full_df01 = pd.read_csv(file_in_path01)\n",
    "print(f'Dataframe shape: {slct_tbl_full_df01.shape}')\n",
    "display(slct_tbl_full_df01.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064029d1-d255-4cd4-b124-8d2fd98be38a",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638aba9-0192-4877-8cf7-927307a05917",
   "metadata": {},
   "source": [
    "### Count missing `article_text` feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc2d34-1bac-44a9-9e3b-46efe38ad671",
   "metadata": {},
   "source": [
    "The majority of null values appear in the `content` column. There are also several in `author` and one in `article_text`. Neither `content` nor `author` will be used for current modeling efforts, therefore they are not a factor. The one instance with missing article text will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "023f3505-3e5a-4675-8fda-dd795da00113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values present: text_id           0\n",
      "source_name       0\n",
      "author          216\n",
      "title             0\n",
      "url               0\n",
      "publish_date      0\n",
      "article_text      2\n",
      "content           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "count_nan = slct_tbl_full_df01.isnull().sum()\n",
    " \n",
    "# printing the number of values present\n",
    "# in the column\n",
    "print('Number of NaN values present: ' + str(count_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0406b0-bdd5-4527-a603-a787c2513864",
   "metadata": {},
   "source": [
    "### Count blank `article_text` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af713abe-82b2-4132-9414-693d71ad3b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>article_text</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20850</th>\n",
       "      <td>22986</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>Fox News Staff</td>\n",
       "      <td>Mug shots of the week: June 11-17, 2023</td>\n",
       "      <td>https://www.foxnews.com/us/mug-shots-week-june...</td>\n",
       "      <td>2023-06-18T00:00:51Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>These mug shots were taken for arrests made th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34174</th>\n",
       "      <td>66130</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>Fox News Staff</td>\n",
       "      <td>Mug shots of the week: June 18-24, 2023</td>\n",
       "      <td>https://www.foxnews.com/us/mug-shots-week-june...</td>\n",
       "      <td>2023-06-25T00:00:48Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>These mug shots were taken for arrests made th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_id source_name          author  \\\n",
       "20850    22986    Fox News  Fox News Staff   \n",
       "34174    66130    Fox News  Fox News Staff   \n",
       "\n",
       "                                         title  \\\n",
       "20850  Mug shots of the week: June 11-17, 2023   \n",
       "34174  Mug shots of the week: June 18-24, 2023   \n",
       "\n",
       "                                                     url  \\\n",
       "20850  https://www.foxnews.com/us/mug-shots-week-june...   \n",
       "34174  https://www.foxnews.com/us/mug-shots-week-june...   \n",
       "\n",
       "               publish_date article_text  \\\n",
       "20850  2023-06-18T00:00:51Z          NaN   \n",
       "34174  2023-06-25T00:00:48Z          NaN   \n",
       "\n",
       "                                                 content  \n",
       "20850  These mug shots were taken for arrests made th...  \n",
       "34174  These mug shots were taken for arrests made th...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(slct_tbl_full_df01[slct_tbl_full_df01['article_text']==None]))\n",
    "display(slct_tbl_full_df01[slct_tbl_full_df01['article_text'].isna()].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3227f0a-dead-400d-b7e5-9e71605d4a0d",
   "metadata": {},
   "source": [
    "### Remove missing `article_text` row(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b240a7-c114-46dd-abd7-9d0abfd38192",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Drop missing citation:\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference\n",
    "/api/pandas.DataFrame.dropna.html#pandas.DataFrame.dropna'''\n",
    "slct_tbl_full_df02 = slct_tbl_full_df01.dropna(subset=['article_text'])\n",
    "print(f'Dataframe shape: {slct_tbl_full_df02.shape}')\n",
    "display(slct_tbl_full_df02.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b5bc0-c639-4b71-9991-7e435ba7e768",
   "metadata": {},
   "source": [
    "### Count characters and words for initial review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8053ca17-e7a9-4f32-be0d-f57960a75d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(ncols=50)  # can use tqdm_gui, optional kwargs, etc\n",
    "# Now you can use `progress_apply` instead of `apply`\n",
    "\n",
    "# Raw text character and word counts\n",
    "slct_tbl_full_df02['char_cnt'] = slct_tbl_full_df02['article_text']\\\n",
    ".progress_apply(len)\n",
    "slct_tbl_full_df02['word_cnt'] = slct_tbl_full_df02['article_text']\\\n",
    ".progress_apply(lambda x: len(x.split()))\n",
    "display(slct_tbl_full_df02.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5530b3c-6fa3-4bff-8530-ac8fcd278ced",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf15b3-0322-4770-b2e9-8785a89457f0",
   "metadata": {},
   "source": [
    "Stats are displayed for both categorical and numerical columns. As expected \"Fox News\" is the most frequent value in `source_name` as the most articles were collected from that news site. The inclusion of \"Associated Press\" as the mode for `author` identified it as a potential source for skew in the final results, as AP was rated as a \"center\" source in the AllSide Media Bias Chart. As a result, all articles with an `author` value of \"Associated Press\" were removed; similarily, articles by \"msn\" and \"Reuters\" were also removed. \\\n",
    "For the numerical values, there was a very large range for both character and word counts (80,454 and 14,306, respectively), but also a large delta between the 75% percentile and max (74,920.5 and 13,433, respectively), indicating a distribution with a very long right tail with a very small amount of some extremely long (outlier) articles. As a result, the standard deviation was also quite large relative to the mean. For the current analyses, no additional efforts will be performed to account for outliers, but this will be an examination factor for future expansion/comparative studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02483fc7-e043-4ad0-85fa-556e6c00805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df02[['source_name',\n",
    "                    'author',\n",
    "                    'publish_date',\n",
    "                    'article_text']].describe(include=\"O\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8cc29-4335-4d23-8bb0-a5aa007b6ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df02.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635c1f0-1c0e-49ab-b804-7952d3d26ba3",
   "metadata": {},
   "source": [
    "### Display Source counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462696cb-d311-4045-bca1-6d6858e44cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df02['source_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9750a38c-5e05-40bc-9e76-ac1d0628e7f7",
   "metadata": {},
   "source": [
    "### Examine inclusion of \"centrist\" sources indicated by `author` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2935e2-506f-44e4-b1df-c7c065042573",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df02a = slct_tbl_full_df02[slct_tbl_full_df02['author']\\\n",
    "                                         .isin(['msn',\n",
    "                                                'Associated Press',\n",
    "                                                'Reuters'])]\n",
    "\n",
    "display(slct_tbl_full_df02a[slct_tbl_full_df02a['author']=='msn'])\n",
    "\n",
    "display(slct_tbl_full_df02a.groupby(by=['source_name', 'author']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12be800-e783-42a0-89e3-70050ac333ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(slct_tbl_full_df02['author'])\n",
    "\n",
    "word_cutoff = 5\n",
    "con_feature_words = set()\n",
    "\n",
    "for word, count in counter.items():\n",
    "    if count > word_cutoff:\n",
    "        con_feature_words.add(word)\n",
    "        \n",
    "print(f'''With a word cutoff of {word_cutoff}, we have \n",
    "{len(con_feature_words)} words as features in the model.''')\n",
    "print(con_feature_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb32e2-9459-41b6-948b-18720564de2c",
   "metadata": {},
   "source": [
    "### Assign class based on `source_name` and AllSides Media Bias Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870cad1-2c19-4702-a223-d1078aeebd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03 = slct_tbl_full_df02[~slct_tbl_full_df02['author']\\\n",
    "                                        .isin(['msn',\n",
    "                                               'Associated Press',\n",
    "                                               'Reuters'])]\n",
    "slct_tbl_full_df03 = slct_tbl_full_df03.reset_index()\n",
    "slct_tbl_full_df03['political_lean'] = 'right'\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "slct_tbl_full_df03.loc[(slct_tbl_full_df03['source_name'] \\\n",
    "                        == 'The Washington Post') \\\n",
    "                       | (slct_tbl_full_df03['source_name'] \\\n",
    "                          == 'CNN'), 'political_lean'] = 'left'\n",
    "\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "display(slct_tbl_full_df03['political_lean'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a207f10-8f48-4dc0-9f44-69eb83fe1730",
   "metadata": {},
   "source": [
    "### Visualize class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceac425-ba86-48d2-a1b6-c21ca64a50b7",
   "metadata": {},
   "source": [
    "There is definitely an imbalance in the number of instances in each class. This is due to Fox News being the most prolific source, whether because they put out a lot more articles or their sites were more consistently available for scraping. This imbalance is not considered extreme and will not be adjusted for within the scope of the current study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8cb5b-a6d0-401d-b6ec-860bf7a1c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['political_lean'].value_counts().plot(kind=\"bar\",\n",
    "                                                         legend=True,\n",
    "                                                         figsize=(5,6),\n",
    "                                                     title='Class distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e8733-2b73-42b4-9185-97a7b5c44109",
   "metadata": {},
   "source": [
    "### Plot word counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e34471-55a9-41b4-9410-e7952fdb3e80",
   "metadata": {},
   "source": [
    "All sources seem to have very similar consolidation of most frequent word counts between 0 and 2,000. However, the two \"left\" sources (CNN and The Washington Post) seem to be the significant source of the outliers, with a small amount of articles each that have extremely large word counts (*note:* the x-axis range was truncated at 6,000 to make it more readabla--as noted above, there were some articles with word counts greater than 14,000). Given the simialirties between the sources within each class, the differences may correlate to intentional word limitation based on perceived audience desires, but in the very least do add evidence that the sources have been grouped together appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee345790-72a0-4242-9567-cd87798b8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03.groupby('source_name')['word_cnt'].plot(kind=\"hist\",\n",
    "                                                           density=True,\n",
    "                                                           alpha=0.5,\n",
    "                                                           legend=True,\n",
    "                                                           figsize=(15,9),\n",
    "                                    title='Histogram of Word Count Frequencies',\n",
    "                                                           xlim=(0,6000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f14e9-07b8-41e9-aea3-2d0753dd631d",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab3518-082e-4b50-b9ce-a51054572417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniq_tok(df_col=None):\n",
    "    '''Display all unique tokens across all instances'''\n",
    "    df_cols1 = pd.Series(df_col)\n",
    "\n",
    "    all_tokens_lst01 = []\n",
    "\n",
    "    [all_tokens_lst01.append(f) for f in df_cols1]\n",
    "    all_tokens_lst01 = list(itertools.chain.from_iterable(all_tokens_lst01))\n",
    "    all_tokens_set01 = set(all_tokens_lst01)\n",
    "    print(len(sorted(all_tokens_set01)))\n",
    "    print(sorted(all_tokens_set01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf76037-6948-4f58-b38f-ce07f417c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df04 = slct_tbl_full_df03.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc903dba-da13-45e1-8034-c435627741ee",
   "metadata": {},
   "source": [
    "### Case-loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246411c2-ec95-4865-863c-c56152cd88a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['lower'] = slct_tbl_full_df03['article_text']\\\n",
    ".apply(str.lower)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9b77a-262f-4a6c-8966-b019f93d2e9f",
   "metadata": {},
   "source": [
    "### Text normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6a6af1-5594-473d-a61c-2f6e782f7108",
   "metadata": {},
   "source": [
    "#### Create function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ddef6-ba90-46a8-b839-e2aca8cb5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6987b7-2c1e-4adc-9525-6e2a6320c8e6",
   "metadata": {},
   "source": [
    "#### Call function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c11dc-07a0-483a-b6a4-093047ab4432",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['norm'] = slct_tbl_full_df03['lower'].apply(normalize)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,1):\n",
    "    try:\n",
    "        print(slct_tbl_full_df03['norm'][c], '\\n')\n",
    "    except:\n",
    "        print(f'Skip {c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96614d-84e9-475f-b4d5-3af952fab092",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2find_rex = rex.compile(r'(click here to get the fox news app)')\n",
    "test_lst = []\n",
    "\n",
    "def test(text):\n",
    "    test_lst.append(text2find_rex.findall(text))\n",
    "\n",
    "slct_tbl_full_df03['norm'].apply(test)\n",
    "\n",
    "display(slct_tbl_full_df03.head())\n",
    "#print(test_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb11e0e-f747-4b07-961b-c402c68d592a",
   "metadata": {},
   "source": [
    "### Remove special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eccd05-df97-464c-924e-116f0d9be165",
   "metadata": {},
   "source": [
    "#### Create function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1558f34b-56f9-4abd-831d-84c97be05263",
   "metadata": {},
   "outputs": [],
   "source": [
    "rex_sep = rex.compile(r'&nbsp;')\n",
    "rex_ucode = rex.compile(r'[\\\\]u20*')\n",
    "\n",
    "'''re.sub lambda citation:\n",
    "https://chat.openai.com/share/402ec66e-2802-4cda-af8c-6f9f5b097d85\n",
    "'''\n",
    "sep_lst = []\n",
    "ucode_lst = []\n",
    "# Add leading and trailing space to URLs\n",
    "def rex_replace(text):\n",
    "    #txt = str(text)\n",
    "    #print(lambda x: x.replace('&nbsp;', ' '))\n",
    "    #sep_lst.append(rex_sep.findall(txt))\n",
    "    #ucode_lst.append(rex_ucode.findall(txt))\n",
    "    text = text.replace(r'&nbsp;', ' ').replace(r'-', ' ')\\\n",
    "    .replace(r'\\n', ' ').replace('\\u2063', ' ').replace('\\u2066', ' ')\\\n",
    "    .replace('\\u2069', ' ').replace('\\u200b', ' ').replace('\\u200d', ' ')\\\n",
    "    .replace('(click to view)', ' ')\\\n",
    "    .replace('a post shared by', ' ')\\\n",
    "    .replace('app users click here', ' ')\\\n",
    "    .replace('app users: click here', ' ')\\\n",
    "    .replace('app users, click here:', ' ')\\\n",
    "    .replace('click here.', ' ')\\\n",
    "    .replace('click here for more cartoons', ' ')\\\n",
    "    .replace('click here for more', ' ')\\\n",
    "    .replace('click here for more sports coverage on foxnews.com', ' ')\\\n",
    "    .replace('click here for other fox news digital adoptable pets stories', ' ')\\\n",
    "    .replace('click here for the fox news app', ' ')\\\n",
    "    .replace('click here for the latest fox news reporting', ' ')\\\n",
    "    .replace('click here for topline and cross tabs conducted', ' ')\\\n",
    "    .replace('click here to hear more', ' ')\\\n",
    "    .replace('click here to ge the fox news app', ' ')\\\n",
    "    .replace('click here to get the fox news app', ' ')\\\n",
    "    .replace('click here to get the opinion newsletter', ' ')\\\n",
    "    .replace('click here to learn more', ' ')\\\n",
    "    .replace('click here to read more', ' ')\\\n",
    "    .replace('click here to sign up for our health newsletter', ' ')\\\n",
    "    .replace('click here to sign up for our lifestyle newsletter', ' ')\\\n",
    "    .replace('click here to sign up for our opinion newsletter', ' ')\\\n",
    "    .replace('click here to sign up for the entertainment newsletter', ' ')\\\n",
    "    .replace('click here to subscribe and get your first year of fox nation free of charge', ' ')\\\n",
    "    .replace('click here to view', ' ')\\\n",
    "    .replace(\"click to get kurt's cyberguy newsletter with quick tips, tech reviews, security alerts and easy how to's to make you smarter\", ' ')\\\n",
    "    .replace(\"click to get kurt's cyberguy newsletter with security alerts, quick tips, tech reviews, security and easy how to's to make you smarter\", ' ')\\\n",
    "    .replace(\"click to get kurt's free cyberguy newsletter with quick tips, tech reviews, security alerts and easy how to's to make you smarter\", ' ')\\\n",
    "    .replace(\"click to get kurt's free cyberguy newsletter with security alerts, quick tips, tech reviews, and easy how to's to make you smarter\", ' ')\\\n",
    "    .replace('click to get the fox news app', ' ')\\\n",
    "    .replace('fox news digital', ' ')\\\n",
    "    .replace('request for comment', ' ')\\\n",
    "    .replace('the ap ', ' ')\\\n",
    "    .replace('copyright © 2023 breitbart', ' ')\\\n",
    "    .replace('all rights reserved', ' ')\\\n",
    "    .replace('copyright 2023 cyberguy.com', ' ')\\\n",
    "    .replace('copyright 2023 fox news network', ' ')\\\n",
    "    .replace('copyright 2023 viq media transcription', ' ')\\\n",
    "    .replace(\"please let us know if you're having issues with commenting\", ' ')\\\n",
    "    .replace('view this post on instagram', ' ')\n",
    "    #txt = txt\n",
    "    #text = text.replace(r'200b', 'd171c')\n",
    "    #text = rex_ucode.sub('', text)\n",
    "    return text\n",
    "\n",
    "    #.replace('philip bump', ' ')\\\n",
    "    #.replace('paul kane', ' ')\\\n",
    "    #.replace('&amp', ' ')\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41145e60-ec69-4513-926e-d62161d7b886",
   "metadata": {},
   "source": [
    "#### Call function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1f8fd-4a04-495f-9542-170393b2c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['replace'] = slct_tbl_full_df03['norm'].apply(rex_replace)\n",
    "\n",
    "#print(ucode_lst)\n",
    "#print(sep_lst)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c215d9-2ecf-466e-b184-312cbb97e1ad",
   "metadata": {},
   "source": [
    "'''Complex citation (add lambda):\n",
    "https://chat.openai.com/share/a135754c-c38c-47ea-8f83-54d41d5397ab\n",
    "'''\n",
    "slct_tbl_full_df03['replace'] = slct_tbl_full_df03['norm'].apply(lambda x: x.replace('&nbsp;', ' ').replace(r'\\n', ' ').replace('\\u2063', ' ').replace('\\u2066', ' ').replace('\\u2069', ' ').replace('\\u200b', ' ').replace('\\u200d', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1e4bc5-f53b-48a4-b878-5a4c40488181",
   "metadata": {},
   "source": [
    "### URL RegEx find"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8a9c5c-095b-45ed-a7a1-3535f1f8ac9a",
   "metadata": {},
   "source": [
    "#### Create function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7720f-00f5-4343-9c38-788623dc091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rex_url_c = rex.compile(r'http[s]?:[\\/]+[\\S]*\\s')\n",
    "\n",
    "'''re.sub lambda citation:\n",
    "https://chat.openai.com/share/402ec66e-2802-4cda-af8c-6f9f5b097d85\n",
    "'''\n",
    "# Add leading and trailing space to URLs\n",
    "def rex_url(text):\n",
    "    text = rex_url_c.sub(lambda match: ' ' + match.group(0) + ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596ac43-1521-45ef-8cdb-81ef0f761dc4",
   "metadata": {},
   "source": [
    "#### Call function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45c0a5-ec10-4c95-9eb7-4cb3842c1de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['rex_urls'] = slct_tbl_full_df03['replace'].apply(rex_url)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1603dece-eae5-4187-a25c-adbc4d990c90",
   "metadata": {},
   "source": [
    "### Separate emojis as individual tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d3f539-4681-4602-a7ea-1673ac5cc8cc",
   "metadata": {},
   "source": [
    "#### Create function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c56e6-c390-45cb-a4f6-f61d3e0768b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_split(text):\n",
    "    return(\"\".join([' ' + c + ' ' if emoji.is_emoji(c) else c for c in text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41e1cdd-5b22-4dd8-9fdf-85e9afa5a076",
   "metadata": {},
   "source": [
    "#### Call function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef37e5-52ee-4241-b049-c28d9b261756",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['emoji_split'] = slct_tbl_full_df03['rex_urls']\\\n",
    ".apply(emoji_split)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,1):\n",
    "    try:\n",
    "        print(slct_tbl_full_df03['emoji_split'][c], '\\n')\n",
    "    except:\n",
    "        print(f'Skip {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd9946-022f-4e74-9396-6b90fe015b90",
   "metadata": {},
   "source": [
    "### Lemmatization using spaCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda3e70-aae9-43da-9b33-7892ed52c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_trans = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemma(text):\n",
    "    trans_txt = nlp_trans(text)\n",
    "    tokens = [t.lemma_ for t in trans_txt]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bae2a11-da0a-4a65-88f1-59f630bc9dd9",
   "metadata": {},
   "source": [
    "slct_tbl_full_df03['lemma'] = slct_tbl_full_df03['replace'].progress_apply(lemma)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,1):\n",
    "    try:\n",
    "        print(slct_tbl_full_df03['lemma'][c], '\\n')\n",
    "    except:\n",
    "        print(f'Skip {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b5fde4-5525-4a81-88e0-6905f95bbc84",
   "metadata": {},
   "source": [
    "#### Display globally unqiue tokens on lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab384e53-4d7c-4fcf-9ccc-0b0ef24575d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniq_tok(df_col=slct_tbl_full_df03['lemma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbfaec-c4cc-4386-89ce-02870d1bbdef",
   "metadata": {},
   "source": [
    "### Split text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef5c58-d225-4f48-b61f-1b7e4e469d7c",
   "metadata": {},
   "source": [
    "#### Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cad2a1-a173-4f13-ac71-e3b408959b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['split'] = slct_tbl_full_df03['emoji_split']\\\n",
    ".apply(str.split)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,1):\n",
    "    try:\n",
    "        print(slct_tbl_full_df03['split'][c], '\\n')\n",
    "    except:\n",
    "        print(f'Skip {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77efa83d-9c67-44b1-8422-c700a7ef52a2",
   "metadata": {},
   "source": [
    "#### Display globablly unqiue tokens on first split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60520bd0-1f52-44cf-bbb0-5480f93f6308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniq_tok(df_col=slct_tbl_full_df03['split'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e472d7-dfdd-479f-9c04-f8647e19be52",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3259f79c-f08c-4cd6-964b-328de3fe53b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# Add additional stop words\n",
    "sw.extend(['',\n",
    "           '️',\n",
    "           'arent',\n",
    "           'cannot',\n",
    "           'cant',\n",
    "           'couldnt',\n",
    "           'couldve',\n",
    "           'didnt',\n",
    "           'doesnt',\n",
    "           'dont',\n",
    "           'hadnt',\n",
    "           'hasnt',\n",
    "           'havent',\n",
    "           'hes',\n",
    "           'im',\n",
    "           \"i'm\",\n",
    "           'isnt',\n",
    "           'it’s',\n",
    "           'ive',\n",
    "           '𝚘𝚏',\n",
    "           'mightnt',\n",
    "           'mustnt',\n",
    "           'neednt',\n",
    "           'shant',\n",
    "           'shes',\n",
    "           'shouldnt',\n",
    "           'shouldve',\n",
    "           'thatll',\n",
    "           'theyll',\n",
    "           'theyve',\n",
    "           'wasnt',\n",
    "           'werent',\n",
    "           'whats',\n",
    "           'weve',\n",
    "           'wont',\n",
    "           'wouldnt',\n",
    "           'wouldve',\n",
    "           'yall',\n",
    "           'youd',\n",
    "           'youll',\n",
    "           'youre',\n",
    "           'youve',\n",
    "           \"we'll\",\n",
    "           \"you’re\",\n",
    "           \"you’ve\",\n",
    "           \"you’ll\",\n",
    "           \"you’d\",\n",
    "           \"she’s\",\n",
    "           \"it’s\",\n",
    "           \"that’ll\",\n",
    "           \"don’t\",\n",
    "           \"should’ve\",\n",
    "           \"aren’t\",\n",
    "           \"couldn’t\",\n",
    "           \"didn’t\",\n",
    "           \"doesn’t\",\n",
    "           \"hadn’t\",\n",
    "           \"hasn’t\",\n",
    "           \"haven’t\",\n",
    "           \"isn’t\",\n",
    "           \"mightn’t\",\n",
    "           \"mustn’t\",\n",
    "           \"needn’t\",\n",
    "           \"shan’t\",\n",
    "           \"shouldn’t\",\n",
    "           \"wasn’t\",\n",
    "           \"weren’t\",\n",
    "           \"won’t\",\n",
    "           \"wouldn’t\",\n",
    "           \"i’m\",\n",
    "           \"we’ll\",\n",
    "           'said',\n",
    "           'told',\n",
    "           'according',\n",
    "           'fox',\n",
    "           'news',\n",
    "           'cnn',\n",
    "           'breitbart',\n",
    "           'reuters',\n",
    "           'reporting',\n",
    "           'reported',\n",
    "           #'statement',\n",
    "           #'spoke',\n",
    "           #'next',\n",
    "           #'though',\n",
    "           #'often',\n",
    "           #'story',\n",
    "           #'updated',\n",
    "           #'additional',\n",
    "           #'developments',\n",
    "           #'follow',\n",
    "          ])\n",
    "\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e7c16-9f07-44c8-a352-082621bc6367",
   "metadata": {},
   "source": [
    "#### Create function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5213016-3512-4f11-9842-7dd018013510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sw_remover(tokens):\n",
    "    return [t for t in tokens if t.lower() not in sw]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cd12d3-da6c-4ff9-a777-889b27d8cc1d",
   "metadata": {},
   "source": [
    "#### Call function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d1623-a6dd-491e-a469-0895023bf2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['no_sw'] = slct_tbl_full_df03['split'].apply(sw_remover)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,1):\n",
    "    print(slct_tbl_full_df03['no_sw'][c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbefe6a-cd44-4e84-9b55-4f9583a0efeb",
   "metadata": {},
   "source": [
    "#### Display no stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87415a4a-1aa5-4e9c-ad0c-51fdcc68bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniq_tok(df_col=slct_tbl_full_df03['no_sw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf622a-0107-4c50-a905-bc218f1f46c4",
   "metadata": {},
   "source": [
    "### Rejoin semi-processed tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c883941-477e-489c-9f82-8e779ac89b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['no_sw_join'] = slct_tbl_full_df03['no_sw'].apply(\" \".join)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,1):\n",
    "    print(slct_tbl_full_df03['no_sw_join'][c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03b56f-4e9f-4908-8cff-b171864b8b98",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf3c8e6-4f16-46e4-aecc-7fa83385918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison\n",
    "#print(punctuation)\n",
    "\n",
    "# Add special hyphen mark\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "#print(tw_punct)\n",
    "\n",
    "# Remove hash and at symbols for later capture of hashtag info\n",
    "tw_punct = tw_punct - {\"@\"}\n",
    "tw_punct = tw_punct - {\"-\"}\n",
    "#tw_punct = tw_punct - {\"/\"}\n",
    "tw_punct.add(\"’\")\n",
    "tw_punct.add(\"‘\")\n",
    "tw_punct.add(\"”\")\n",
    "tw_punct.add(\"“\")\n",
    "tw_punct.add(\"…\")\n",
    "tw_punct.add(\"—\")\n",
    "tw_punct.add(\"...\")\n",
    "tw_punct.add(\"€\")\n",
    "tw_punct.add(\"±\")\n",
    "tw_punct.add(\"£\")\n",
    "tw_punct.add(\"¡\")\n",
    "tw_punct.add(\"§\")\n",
    "tw_punct.add(\"⦿\")\n",
    "\n",
    "print(tw_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95975adf-5057-41b4-a16a-4b4750967f64",
   "metadata": {},
   "source": [
    "#### Create function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c415b-7b1a-4412-962b-071119ad0058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text, punct_set=tw_punct): \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03675f39-bb3a-4c82-a529-7bcbe43815fd",
   "metadata": {},
   "source": [
    "#### Call function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e3506-9b17-4f56-aae2-16e15c054f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['no_sw_join_no_punc'] = slct_tbl_full_df03['no_sw_join']\\\n",
    ".apply(remove_punctuation, punct_set=tw_punct)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,1):\n",
    "    try:\n",
    "        print(slct_tbl_full_df03['no_sw_join_no_punc'][c], '\\n')\n",
    "    except:\n",
    "        print(f'\\nerror on {c}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7644f8-60b3-48e9-aa05-2164563610bf",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca4d11-4ef6-40ae-997f-ba7860f7a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['no_sw_join_no_punc_tok'] \\\n",
    "= slct_tbl_full_df03['no_sw_join_no_punc'].apply(str.split)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,1):\n",
    "    print(slct_tbl_full_df03['no_sw_join_no_punc_tok'][c], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53bc528-38ed-4140-aeb2-2c70986ea7a3",
   "metadata": {},
   "source": [
    "#### Display globally unqiue tokens on final tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7963d626-18db-4d88-a58c-7c9c0ca88380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniq_tok(df_col=slct_tbl_full_df03['no_sw_join_no_punc_tok'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7fff5-d031-43a3-9688-eb37bff61ea8",
   "metadata": {},
   "source": [
    "### Pipeline consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d81812-6494-49a4-93de-32f0ce5e36f6",
   "metadata": {},
   "source": [
    "#### Pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d0ddc-25a8-4099-a987-b32556cda9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(text, pipeline):\n",
    "    '''Run a pipeline of text processing transformers'''\n",
    "    tokens = str(text)\n",
    "    \n",
    "    # Pull key and val from trans dictionaries\n",
    "    for transformer in pipeline:\n",
    "        trans = list(transformer.keys())[0]\n",
    "        args = list(transformer.values())[0]\n",
    "        #print(trans)\n",
    "        #print(args)\n",
    "        if args == None:\n",
    "            #print(1)\n",
    "            tokens = trans(tokens)\n",
    "        else:\n",
    "            #print('check99', trans, args)\n",
    "            tokens = trans(tokens, args) \n",
    "\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba9c40f-2e73-4cfb-9d3e-a3fb567d47af",
   "metadata": {},
   "source": [
    "#### `article_text` preprocessing - w/o lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe43d935-bf01-47a5-9a02-bc3d94973ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set transformer pipeline 1:\n",
    "Caseloading, normalization (using textacy), special ch removal,\n",
    "split on whitespace, stop word removal, rejoin,\n",
    "remove custom punctuation, tokenize\n",
    "'''\n",
    "transformers01 = [{str.lower: None},\n",
    "                  {normalize: None},\n",
    "                  {rex_replace: None},\n",
    "                  {rex_url: None},\n",
    "                  {emoji_split: None},\n",
    "                  {str.split: None},\n",
    "                  {sw_remover: None},\n",
    "                  {\" \".join: None},\n",
    "                  {remove_punctuation: tw_punct},\n",
    "                  {str.split: None},\n",
    "                  {\" \".join: None},\n",
    "                 ]\n",
    "\n",
    "# Apply transformers to pandas dataframe, w/ new col containing tokens\n",
    "slct_tbl_full_df04['processed_text'] = slct_tbl_full_df04['article_text']\\\n",
    ".progress_apply(prepare, pipeline=transformers01)\n",
    "\n",
    "slct_tbl_full_df04['processed_text_split'] = slct_tbl_full_df04['processed_text']\\\n",
    ".progress_apply(str.split)\n",
    "\n",
    "slct_tbl_full_df04['num_tokens'] = slct_tbl_full_df04['processed_text_split']\\\n",
    ".map(len)\n",
    "\n",
    "display(slct_tbl_full_df04.head())\n",
    "\n",
    "# Review unique tokens across entire dataset\n",
    "for c in range(0,1):\n",
    "    try:\n",
    "        print(slct_tbl_full_df04['processed_text'][c], '\\n')\n",
    "    except:\n",
    "        print(f'Skip {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc22a12-a22a-472d-86d6-594aa1276119",
   "metadata": {},
   "source": [
    "##### Display globally unqiue tokens on final tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf661de-b023-41c5-8dbb-34e65d2c5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniq_tok(df_col=slct_tbl_full_df04['processed_text_split'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add57a6e-edf7-4dbf-8c0d-114692380d7f",
   "metadata": {},
   "source": [
    "#### `article_text` preprocessing - w/ lemmatization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6610955f-5cdc-46e8-9b58-a4cb7e74b255",
   "metadata": {},
   "source": [
    "'''Set transformer pipeline 2:\n",
    "Caseloading, normalization (using textacy), special ch removal,\n",
    "lemmitization, stop word removal, rejoin,\n",
    "remove custom punctuation, tokenize\n",
    "'''\n",
    "transformers02 = [{str.lower: None},\n",
    "                  {normalize: None},\n",
    "                  {rex_replace: None},\n",
    "                  {lemma: None},\n",
    "                  {\" \".join: None},\n",
    "                  {rex_url: None},\n",
    "                  {emoji_split: None},\n",
    "                  {str.split: None},\n",
    "                  {sw_remover: None},\n",
    "                  {\" \".join: None},\n",
    "                  {remove_punctuation: tw_punct},\n",
    "                  {str.split: None},\n",
    "                  {\" \".join: None},\n",
    "                 ]\n",
    "\n",
    "# Apply transformers to pandas dataframe, w/ new col containing tokens\n",
    "slct_tbl_full_df04['processed_lemmas'] = slct_tbl_full_df04['article_text']\\\n",
    ".progress_apply(prepare, pipeline=transformers02)\n",
    "\n",
    "slct_tbl_full_df04['processed_lemmas_split'] = slct_tbl_full_df04['processed_lemmas']\\\n",
    ".progress_apply(str.split)\n",
    "\n",
    "slct_tbl_full_df04['num_lemmas'] = slct_tbl_full_df04['processed_lemmas_split']\\\n",
    ".map(len)\n",
    "\n",
    "display(slct_tbl_full_df04.head())\n",
    "\n",
    "# Review unique tokens across entire dataset\n",
    "for c in range(0,1):\n",
    "    try:\n",
    "        print(slct_tbl_full_df04['processed_lemmas'][c], '\\n')\n",
    "    except:\n",
    "        print(f'Skip {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa0599-07ee-4378-a84a-5ce43f90d573",
   "metadata": {},
   "source": [
    "##### Display globally unqiue tokens on final tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728821c3-b6f5-40b4-9b21-159bc4c59989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniq_tok(df_col=slct_tbl_full_df04['processed_lemmas_split'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f6e5c8-9f2e-40a9-aecf-a02872a05c3e",
   "metadata": {},
   "source": [
    "### Calculate concentration ratio of each set of corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e0d707-1f72-483c-8278-40ae9904ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(slct_tbl_full_df04['political_lean'].value_counts())\n",
    "\n",
    "slct_tbl_full_df04_left = slct_tbl_full_df04[\\\n",
    "                                             slct_tbl_full_df04[\\\n",
    "                                                'political_lean'] == 'left']\n",
    "\n",
    "print(slct_tbl_full_df04_left.shape)\n",
    "#display(slct_tbl_full_df04_left.head())\n",
    "\n",
    "slct_tbl_full_df04_right = slct_tbl_full_df04[\\\n",
    "                                              slct_tbl_full_df04[\\\n",
    "                                                'political_lean'] == 'right']\n",
    "\n",
    "print(slct_tbl_full_df04_right.shape)\n",
    "#display(slct_tbl_full_df04_right.head())\n",
    "\n",
    "slct_tbl_full_df04_left_s1 = list(itertools.chain.from_iterable(\n",
    "    list(pd.Series(slct_tbl_full_df04_left['processed_text_split']))))\n",
    "print(slct_tbl_full_df04_left_s1[:10])\n",
    "slct_tbl_full_df04_right_s1 = list(itertools.chain.from_iterable(\n",
    "    list(pd.Series(slct_tbl_full_df04_right['processed_text_split']))))\n",
    "print(slct_tbl_full_df04_right_s1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e01554-cc8e-4032-a8f1-9c2538f6ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capstone note: Extended custom function to use bi- and tri-grams\n",
    "def concen_ratio(artist_lst=[],\n",
    "                 lsts=[]):\n",
    "    lyr_corp_lst = []\n",
    "    for l in lsts:\n",
    "        print(type(l))\n",
    "        lyr_corp_lst.append(' '.join(l))\n",
    "    print(len(lyr_corp_lst))\n",
    "    #print(lyr_corp_lst)\n",
    "\n",
    "    cv = CountVectorizer(input='content',\n",
    "                         encoding='utf-8',\n",
    "                         stop_words=None,\n",
    "                         token_pattern=r'\\S+',\n",
    "                         ngram_range=(1,3)\n",
    "                        )\n",
    "\n",
    "    lyr_tokens_fit = cv.fit(lyr_corp_lst)\n",
    "\n",
    "    print(pd.Series(cv.get_feature_names_out()).sample(15))\n",
    "\n",
    "    lyr_tokens_sm = cv.transform(lyr_corp_lst)\n",
    "    display(lyr_tokens_sm)\n",
    "\n",
    "    df = pd.DataFrame(lyr_tokens_sm.toarray(),\n",
    "                      columns=cv.get_feature_names_out())\n",
    "    #display(df)\n",
    "\n",
    "    df02 = df.copy()\n",
    "    df02['r_sum'] = df02.sum(axis=1)\n",
    "    #display(df02)\n",
    "\n",
    "    '''Filter by frequency for all columns citation:\n",
    "    OpenAI. (2021). ChatGPT [Computer software]. https://openai.com/;\n",
    "    https://pandas.pydata.org/pandas-docs/stable/reference/api/\n",
    "    pandas.DataFrame.ge.html'''\n",
    "    condition = df.ge(5).all()\n",
    "    #print(condition)\n",
    "\n",
    "    # Get the list of columns that satisfy the condition\n",
    "    columns = condition[condition].index.tolist()\n",
    "    #print(columns)\n",
    "    columns.append('r_sum')\n",
    "    #print(columns)\n",
    "\n",
    "    #display(df02[columns])\n",
    "\n",
    "    df03 = df02[columns].copy()\n",
    "    display(df03)\n",
    "\n",
    "    '''Filter by frequency for all columns & add summary row citation:\n",
    "    OpenAI. (2021). ChatGPT [Computer software]. https://openai.com/'''\n",
    "    df04 = df03.apply(lambda x: x / df03.iloc[:,-1], axis=0)\n",
    "    #display(df04)\n",
    "\n",
    "    # Create new rows by dividing one artist row by the second artists row\n",
    "    new_row01 = df04.iloc[0] / df04.iloc[1]\n",
    "    new_row02 = df04.iloc[1] / df04.iloc[0]\n",
    "\n",
    "    # Append the new row to the DataFrame\n",
    "    df04 = df04.append(new_row01, ignore_index=True)\n",
    "    df04 = df04.append(new_row02, ignore_index=True)\n",
    "    display(df04)\n",
    "\n",
    "    # Transpose dataframe\n",
    "    df05 = df04.T\n",
    "    df05 = df05.reset_index()\n",
    "    df05.columns = ['token',\n",
    "                    'c1_concen',\n",
    "                    'c2_concen',\n",
    "                    'c1c2_concen_ratio',\n",
    "                    'c2c1_concen_ratio']\n",
    "    #print(df05)\n",
    "    \n",
    "    '''Sort values citation:\n",
    "    https://pandas.pydata.org/pandas-docs/stable/reference/api\n",
    "    /pandas.DataFrame.sort_values.html'''\n",
    "    print(artist_lst[0])\n",
    "    display(df05[['token',\n",
    "                  'c1c2_concen_ratio']].sort_values(by='c1c2_concen_ratio',\n",
    "                                                    ascending=False).head(10))\n",
    "    print(artist_lst[1])\n",
    "    display(df05[['token',\n",
    "                  'c2c1_concen_ratio']].sort_values(by='c2c1_concen_ratio',\n",
    "                                                    ascending=False).head(10))\n",
    "\n",
    "concen_ratio(artist_lst=['Left-Right Concentration Ratio',\n",
    "                         'Right-Left Concentration Ratio'],\n",
    "             lsts=[slct_tbl_full_df04_left_s1,\n",
    "                   slct_tbl_full_df04_right_s1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5ecd7a-9351-4220-9f96-df300639f65e",
   "metadata": {},
   "source": [
    "### KWIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad1d899-658b-4978-b6cf-10292f632076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kwic(doc_series, keyword, window=35, print_samples=5):\n",
    "    '''Search article text for keywords in context (KWIC)'''\n",
    "    def add_kwic(text):\n",
    "        kwic_list.extend(keyword_in_context(doc=text,\n",
    "                                            keyword=keyword,\n",
    "                                            ignore_case=True,\n",
    "                                            window_width=window))\n",
    "    kwic_list = []\n",
    "    doc_series.map(add_kwic)\n",
    "    \n",
    "    if print_samples is None or print_samples==0:\n",
    "        return kwic_list\n",
    "    else:\n",
    "        k = min(print_samples, len(kwic_list))\n",
    "        print(f\"{k} random samples out of {len(kwic_list)}\" + \\\n",
    "              f\" contexts for '{keyword}':\")\n",
    "        for sample in random.sample(list(kwic_list), k):\n",
    "            print(re.sub(r'[\\n\\t]', ' ', sample[0]) + ' ' +\\\n",
    "                  sample[1] + ' ' +\\\n",
    "                  re.sub(r'[\\n\\t]', ' ', sample[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0020b8-e559-415c-a80d-8ec8d7144adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kwic(slct_tbl_full_df03['article_text'], 'amp', window=150, print_samples=120)\n",
    "\n",
    "kwic(slct_tbl_full_df03['norm'], 'economic', window=50, print_samples=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee1f51-18e2-493f-a650-59ab528ba808",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c0208-7db2-4f7e-ac26-006f9cd1d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df04['stratifier'] = slct_tbl_full_df04['political_lean']\\\n",
    ".astype(str) + slct_tbl_full_df04['source_name'].astype(str)\n",
    "slct_tbl_full_df04['stratifier'] = slct_tbl_full_df04['stratifier']\\\n",
    ".map(str.lower)\n",
    "display(slct_tbl_full_df04.head())\n",
    "\n",
    "y01a = ['stratifier']\n",
    "slct_tbl_full_df04_y01_vc01a = slct_tbl_full_df04[y01a].to_numpy()\n",
    "print(slct_tbl_full_df04_y01_vc01a.shape)\n",
    "\n",
    "y01 = ['political_lean']\n",
    "slct_tbl_full_df04_y01_vc01 = slct_tbl_full_df04[y01].to_numpy()\n",
    "print(slct_tbl_full_df04_y01_vc01.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4fee5c-dd14-4381-a686-84b01e05f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_train_x01, \\\n",
    "nlm_test_x01, \\\n",
    "nlm_train_y01, \\\n",
    "nlm_test_y01 = train_test_split(slct_tbl_full_df04['processed_text'],\n",
    "                                slct_tbl_full_df04_y01_vc01,\n",
    "                                test_size=.15,\n",
    "                                random_state=1699,\n",
    "                                stratify=slct_tbl_full_df04_y01_vc01a\n",
    "                               )\n",
    "\n",
    "nlm_train_y01 = nlm_train_y01.ravel()\n",
    "nlm_test_y01 = nlm_test_y01.ravel()\n",
    "\n",
    "print(f'{nlm_train_x01.shape}')\n",
    "print(f'{nlm_train_y01.shape}')\n",
    "print(f'\\n{nlm_test_x01.shape}')\n",
    "print(f'{nlm_test_y01.shape}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d41de2d-5f00-4da5-8058-6d4747581e9a",
   "metadata": {},
   "source": [
    "lem_train_x01, \\\n",
    "lem_test_x01, \\\n",
    "lem_train_y01, \\\n",
    "lem_test_y01 = train_test_split(slct_tbl_full_df04['processed_lemmas'],\n",
    "                                slct_tbl_full_df04_y01_vc01,\n",
    "                                test_size=.15,\n",
    "                                random_state=1699,\n",
    "                                stratify=slct_tbl_full_df04_y01_vc01a\n",
    "                               )\n",
    "\n",
    "lem_train_y01 = lem_train_y01.ravel()\n",
    "lem_test_y01 = lem_test_y01.ravel()\n",
    "\n",
    "print(f'{lem_train_x01.shape}')\n",
    "print(f'{lem_train_y01.shape}')\n",
    "print(f'\\n{lem_test_x01.shape}')\n",
    "print(f'{lem_test_y01.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7c7de-5578-4499-8753-b7ca79c59924",
   "metadata": {},
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933eb22-7470-4558-baac-769b009cb28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(slct_tbl_full_df04['processed_text']))\n",
    "display(slct_tbl_full_df04['processed_text'].head())\n",
    "\n",
    "'''\n",
    "BERTopic citation\n",
    "https://chat.openai.com/share/04f961fa-14f3-4bfb-b7ed-f71f9ebaee65'''\n",
    "# Read the dataset into a pandas DataFrame\n",
    "#df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Extract the column containing the unstructured text\n",
    "documents = slct_tbl_full_df04['processed_text']\n",
    "\n",
    "# Train BERTopic on the text column\n",
    "model = BERTopic(ngram_range=(1,3))\n",
    "topics, _ = model.fit_transform(documents)\n",
    "\n",
    "# Assign topics to records and generate labels\n",
    "k = 3  # Number of words to select as labels per record\n",
    "labels_per_record = []\n",
    "for record_topics in topics:\n",
    "    record_labels = []\n",
    "    for topic_id in record_topics:\n",
    "        topic_words = model.get_topic(topic_id)[:k]\n",
    "        record_labels.extend(topic_words)\n",
    "    labels_per_record.append(record_labels)\n",
    "\n",
    "# Print labels for each record\n",
    "for i, labels in enumerate(labels_per_record):\n",
    "    print(f\"Record {i+1}: {labels}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5417844-fe73-4dee-8521-0a96206c9c77",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01de41-528d-4701-8644-690ec31611cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(slct_tbl_full_df04['processed_text'].shape)\n",
    "print(slct_tbl_full_df04['processed_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608ad7a0-6559-45a6-b0ad-c57c33a23bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                            analyzer='word',\n",
    "                            stop_words=sw,\n",
    "                            token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "                            ngram_range=(1,3),\n",
    "                            max_df=.7,\n",
    "                            min_df=5)\n",
    "\n",
    "nlm_train_x01_mtx = nlm_tfidf.fit_transform(nlm_train_x01)\n",
    "nlm_test_x01_mtx = nlm_tfidf.transform(nlm_test_x01)\n",
    "\n",
    "display(nlm_train_x01_mtx)\n",
    "display(nlm_test_x01_mtx)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9dcf0b59-ad10-4326-a3ad-3242f976aafa",
   "metadata": {},
   "source": [
    "lem_tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                            analyzer='word',\n",
    "                            stop_words=sw,\n",
    "                            token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "                            ngram_range=(1,3),\n",
    "                            max_df=.7,\n",
    "                            min_df=5)\n",
    "\n",
    "lem_train_x01_mtx = lem_tfidf.fit_transform(lem_train_x01)\n",
    "lem_test_x01_mtx = lem_tfidf.transform(lem_test_x01)\n",
    "\n",
    "display(lem_train_x01_mtx)\n",
    "display(lem_test_x01_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac6db9e-d7cb-4bb2-9186-8589c1440916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_samp_dwm(sm=None,\n",
    "                     vec=None,\n",
    "                     n=(1,1),\n",
    "                     rs_tup=(1,1)):\n",
    "    mtx_df01 = pd.DataFrame(sm.toarray(),\n",
    "                            columns=vec.get_feature_names_out())\n",
    "\n",
    "    mtx_df01a = mtx_df01.sample(n=n[0],\n",
    "                                random_state=rs_tup[0],\n",
    "                                axis=1)\n",
    "\n",
    "    mtx_df01b = mtx_df01a.sample(n=n[1],\n",
    "                                 random_state=rs_tup[1],\n",
    "                                 axis=0)\n",
    "\n",
    "    display(mtx_df01b)\n",
    "    return vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da48f842-0629-4393-bc7a-82dd18071ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_tup=(1699,1699)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb5508-46f2-4533-a93a-b5e2e6db3e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_train_x01_mtx_cols = display_samp_dwm(sm=nlm_train_x01_mtx,\n",
    "                                          vec=nlm_tfidf,\n",
    "                                          n=(17,11),\n",
    "                                          rs_tup=(5,1699))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45d086-7040-476a-aab9-d8d0d911ce11",
   "metadata": {},
   "source": [
    "display_samp_dwm(sm=lem_train_x01_mtx,\n",
    "                 vec=lem_tfidf,\n",
    "                 n=(17,11),\n",
    "                 rs_tup=(5,1699))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e9cc2-b782-4911-82b6-65af1d6d1792",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52fa56d-6395-41d7-b1eb-6d88cf94a890",
   "metadata": {},
   "source": [
    "### Algorithm setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eac05c-b059-4bc9-8ac2-c048b7d2d15c",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier - Using `BayesSearchCV`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ced0ead9-c25b-4bcc-b307-d4941ab880a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# Start timer script\n",
    "start_time = dt.datetime.today()\n",
    "\n",
    "# Citation: Hochberg, 2018; Shanmukh, 2021\n",
    "m2v1_gbc_pip = Pipeline([('gbc',\n",
    "                          GradientBoostingClassifier(random_state=1699))])\n",
    "\n",
    "loss_hparam = Categorical(['log_loss', 'exponential'])\n",
    "lrate_hparam = Real(1e-3, 1e3, prior='log-uniform')\n",
    "nest_hparam = Integer(1e2, 1e3, prior='log-uniform')\n",
    "msamp_hparam = Real(.01, .95, prior='log-uniform')\n",
    "mdepth_hparam = Integer(1, 20, prior='log-uniform')\n",
    "mfeat_hparam = Categorical(['sqrt', 'log2', None])\n",
    "#wstart_hparam = Categorical([True, False])\n",
    "#calph_hparam = Real(0.0, 100.0, prior='log-uniform')\n",
    "                #'gbc__warm_start': wstart_hparam\n",
    "                #'gbc__ccp_alpha': calph_hparam\n",
    "\n",
    "m2v1_gbc_grd = {'gbc__loss': loss_hparam,\n",
    "                'gbc__learning_rate': lrate_hparam,\n",
    "                'gbc__n_estimators': nest_hparam,\n",
    "                'gbc__min_samples_split': msamp_hparam,\n",
    "                'gbc__max_depth': mdepth_hparam,\n",
    "                'gbc__max_features': mfeat_hparam\n",
    "               }\n",
    "\n",
    "'''Change GBC default scoring from accuracy to F1 score citation:\n",
    "https://chat.openai.com/share/254f382b-4a8e-48e8-acd5-2918f0bbc59d\n",
    "'''\n",
    "f1_scorer = make_scorer(f1_score,\n",
    "                        pos_label='right')\n",
    "\n",
    "'''Customize cross-validation citation:\n",
    "https://machinelearningmastery.com\n",
    "/scikit-optimize-for-hyperparameter-tuning-in-machine-learning/\n",
    "'''\n",
    "cv = RepeatedStratifiedKFold(n_splits=10,\n",
    "                             n_repeats=2,\n",
    "                             random_state=1699)\n",
    "\n",
    "m2v1_gbc = BayesSearchCV(m2v1_gbc_pip,\n",
    "                         m2v1_gbc_grd,\n",
    "                         n_iter=20,\n",
    "                         scoring=f1_scorer,\n",
    "                         cv=cv,\n",
    "                         n_jobs=3,\n",
    "                         refit=True,\n",
    "                         verbose=4,\n",
    "                         random_state=1699)\n",
    "\n",
    "m2v1_gbc.fit(nlm_train_x01_mtx, nlm_train_y01)\n",
    "\n",
    "# End timer script\n",
    "end_time = dt.datetime.today()\n",
    "time_elapse = end_time - start_time\n",
    "print(f'Start Time = {start_time}')\n",
    "print(f'End Time = {end_time}')\n",
    "print(f'Elapsed Time = {time_elapse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4cb7b9-0201-4414-8708-609709aaaaff",
   "metadata": {},
   "source": [
    "### Pickle best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70b2f1-2b41-4bf5-94da-f6154d86c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the pickled model\n",
    "mod_folder_name = 'trained_models'\n",
    "m2v1_pkl_file_name = 'm2v2_gbc.pkl'\n",
    "\n",
    "pkl_file_path01 = os.path.join(curr_dir, mod_folder_name, m2v1_pkl_file_name)\n",
    "\n",
    "print(f'CSV file 1 in path: {pkl_file_path01}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06440afe-ebfd-4354-a6ad-0294e335d144",
   "metadata": {},
   "source": [
    "with open(pkl_file_path01, \"wb\") as file:\n",
    "    pickle.dump(m2v1_gbc, file)\n",
    "\n",
    "print(\"Model pickled and saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e40871-3f48-4494-aa49-8e7072ff0f0b",
   "metadata": {},
   "source": [
    "### Load pickled best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319f4a2-a7dd-4c35-91f1-4d1a5f3e137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file_path01, 'rb') as file:\n",
    "    m2v1_gbc = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80813c6-5b96-4e3c-8962-d1ec59146c29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'\\nBest Estimator:\\n{m2v1_gbc.best_estimator_}')\n",
    "\n",
    "print('\\nCross-validaton results:')\n",
    "display(pd.DataFrame(m2v1_gbc.cv_results_))\n",
    "\n",
    "train_m2v1_gbc_y01_pred = m2v1_gbc.predict_proba(nlm_train_x01_mtx)\n",
    "print(f'\\nFirst 10 train set predictions:\\n{train_m2v1_gbc_y01_pred[:10]}')\n",
    "\n",
    "test_m2v1_gbc_y01_pred = m2v1_gbc.predict_proba(nlm_test_x01_mtx)\n",
    "print(f'\\nFirst 10 test set predictions:\\n{test_m2v1_gbc_y01_pred[:10]}')\n",
    "\n",
    "print(f'\\nBest Score for \"{m2v1_gbc.scorer_}\" is {m2v1_gbc.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6139e3-2832-45f7-ab18-81804bdae0d5",
   "metadata": {},
   "source": [
    "#### Train set check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b81e31-2e73-4eb5-95cd-4bfe965d45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_train_y01_pred = m2v1_gbc.predict(nlm_train_x01_mtx)\n",
    "nlm_train_y01_pred_cm = confusion_matrix(nlm_train_y01, nlm_train_y01_pred)\n",
    "\n",
    "print(classification_report(nlm_train_y01, nlm_train_y01_pred))\n",
    "print(nlm_train_y01_pred_cm)\n",
    "\n",
    "'''Citation:\n",
    "https://scikit-learn.org/stable/modules/generated\n",
    "/sklearn.metrics.ConfusionMatrixDisplay.html\n",
    "#sklearn.metrics.ConfusionMatrixDisplay.plot\n",
    "'''\n",
    "nlm_train_cm_dsp = ConfusionMatrixDisplay(confusion_matrix=nlm_train_y01_pred_cm,\n",
    "                                          display_labels=m2v1_gbc.classes_)\n",
    "nlm_train_cm_dsp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac5279f-c8bf-44fe-bd35-793a415ad6ef",
   "metadata": {},
   "source": [
    "#### ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38396e74-2264-4cb2-a368-6da13ec12717",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_train_y01_pred_decf = m2v1_gbc.decision_function(nlm_train_x01_mtx)\n",
    "RocCurveDisplay.from_predictions(nlm_train_y01, nlm_train_y01_pred_decf,\n",
    "                                 pos_label='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8676a301-5e14-4f32-a8c4-07d109f5ce5a",
   "metadata": {},
   "source": [
    "#### Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049dd720-8407-43df-af04-5660d1f7170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_test_y01_pred = m2v1_gbc.predict(nlm_test_x01_mtx)\n",
    "nlm_test_y01_pred_cm = confusion_matrix(nlm_test_y01, nlm_test_y01_pred)\n",
    "\n",
    "print('Test Set Evaluation Metrics')\n",
    "print(classification_report(nlm_test_y01, nlm_test_y01_pred))\n",
    "print(nlm_test_y01_pred_cm)\n",
    "\n",
    "'''Citation:\n",
    "https://scikit-learn.org/stable/modules/generated\n",
    "/sklearn.metrics.ConfusionMatrixDisplay.html\n",
    "#sklearn.metrics.ConfusionMatrixDisplay.plot\n",
    "'''\n",
    "nlm_test_cm_dsp = ConfusionMatrixDisplay(confusion_matrix=nlm_test_y01_pred_cm,\n",
    "                                         display_labels=m2v1_gbc.classes_)\n",
    "nlm_test_cm_dsp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea7417-3f8f-447f-b6d6-449a8186b12f",
   "metadata": {},
   "source": [
    "#### Variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565c42f-c16b-4b37-a406-0bb455ca055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlm_train_x01_mtx_cols)\n",
    "print(type(nlm_train_x01_mtx_cols))\n",
    "print(nlm_train_x01_mtx_cols.shape)\n",
    "\n",
    "x = m2v1_gbc.best_estimator_.named_steps['gbc'].feature_importances_\n",
    "x_df01 = pd.DataFrame(x, columns=['var_imp'])\n",
    "x_df01['feature'] = nlm_train_x01_mtx_cols\n",
    "x_df02 = x_df01.sort_values(by=['var_imp'], ascending=False)\n",
    "x_df03 = x_df02.head(20)\n",
    "\n",
    "display(x_df02.head())\n",
    "print(type(x_df02))\n",
    "print(x_df02.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83be1392-39d1-493d-8004-05f3e23a0000",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Citation:\n",
    "https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
    "'''\n",
    "# plot feature importance\n",
    "#figure = plt.figsize((10,9))\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Feature Importance (Top 20)')\n",
    "plt.barh([x for x in range(len(x_df03['var_imp']))], x_df03['var_imp'],\n",
    "         tick_label=x_df03['feature'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TNmodel1=nlm_test_y01_pred_cm[0][0]\n",
    "FPmodel1=nlm_test_y01_pred_cm[0][1]\n",
    "FNmodel1=nlm_test_y01_pred_cm[1][0]\n",
    "TPmodel1=nlm_test_y01_pred_cm[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d24a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results:\n",
    "from tabulate import tabulate\n",
    "\n",
    "TANmodel1=TNmodel1+FPmodel1\n",
    "TAPmodel1=TPmodel1+FNmodel1\n",
    "TPPmodel1=FPmodel1+TPmodel1\n",
    "TPNmodel1=TNmodel1+FNmodel1\n",
    "GTmodel1=TANmodel1+TAPmodel1\n",
    "AccuracyM1=(TNmodel1+TPmodel1)/GTmodel1\n",
    "ErrorRateM1=1-AccuracyM1\n",
    "SensitivityM1=TPmodel1/(TAPmodel1)\n",
    "RecallM1=SensitivityM1\n",
    "SpecificityM1=TNmodel1/TANmodel1\n",
    "PrecisionM1=TPmodel1/TPPmodel1\n",
    "F1M1=2*PrecisionM1*RecallM1/(PrecisionM1 + RecallM1)\n",
    "F2M1=5*(PrecisionM1*RecallM1)/((4*PrecisionM1)+RecallM1)\n",
    "Fp5M1=(1.25)*(PrecisionM1*RecallM1)/((0.25*PrecisionM1)+RecallM1)\n",
    "\n",
    "header = [\"Accuracy\", \"Error Rate\", \"Sensitivity\", \"Recall\", \"Specificity\",\n",
    "          \"Precision\", \"F1\", \"F2\", \"F0.5\"]\n",
    "data1 = [[\"Accuracy\", AccuracyM1], [\"Error Rate\", ErrorRateM1],\n",
    "         [\"Sensitivity\", SensitivityM1],\n",
    "         [\"Recall\", RecallM1], [\"Specificity\", SpecificityM1],\n",
    "         [\"Precision\", PrecisionM1],\n",
    "         [\"F1\", F1M1], [\"F2\", F2M1], [\"F0.5\", Fp5M1]]\n",
    "\n",
    "col_names=[\"Measurement\", \"Linear SVC Model\"]\n",
    "\n",
    "ModelEvaluationTable = tabulate(data1, headers=col_names,\n",
    "                                tablefmt=\"fancy_grid\")\n",
    "\n",
    "print(ModelEvaluationTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8552b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_metric_results_TheHill=pd.DataFrame(data1)\n",
    "Data_metric_results_TheHill.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712578eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_metric_results_TheHill.rename (columns = {0:'Measurement'}, inplace=True) \n",
    "Data_metric_results_TheHill.rename (columns = {1:'Result'}, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.bar(x=ModelEvaluationTable)\n",
    "ax=Data_metric_results_TheHill[(Data_metric_results_TheHill['Measurement'] == 'Accuracy') | \n",
    "                            (Data_metric_results_TheHill['Measurement'] == 'Recall') |\n",
    "                            (Data_metric_results_TheHill['Measurement'] == 'F1') |\n",
    "                            (Data_metric_results_TheHill['Measurement'] == 'Error Rate')].plot(kind=\"barh\", \n",
    "                                                                                               x='Measurement',\n",
    "                                  figsize=(5,6),\n",
    "                                  title='Linear SVC Performance metrics on Test Data')\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax.set_xlim(right=1.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503d0e5-eae8-48e1-9f7d-7212a06fe268",
   "metadata": {},
   "source": [
    "## Business problem application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ab6b8-abf6-442d-a6b6-0e52462828a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_df01 = pd.read_csv(file_in_path02)\n",
    "\n",
    "print(center_df01.shape)\n",
    "display(center_df01.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdbcbb8-d869-48d6-a634-3010fd492e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformers to pandas dataframe, w/ new col containing tokens\n",
    "center_df01['processed_text'] = center_df01['article_text']\\\n",
    ".progress_apply(prepare, pipeline=transformers01)\n",
    "\n",
    "center_df01['processed_text_split'] = center_df01['processed_text']\\\n",
    ".progress_apply(str.split)\n",
    "\n",
    "center_df01['num_tokens'] = center_df01['processed_text_split']\\\n",
    ".map(len)\n",
    "\n",
    "display(center_df01.head())\n",
    "\n",
    "# Review unique tokens across entire dataset\n",
    "for c in range(0,1):\n",
    "    try:\n",
    "        print(center_df01['processed_text'][c], '\\n')\n",
    "    except:\n",
    "        print(f'Skip {c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf8a01-d053-40c5-aa71-2caebe2fbca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_apply_x01_mtx = nlm_tfidf.transform(center_df01['processed_text'])\n",
    "\n",
    "print(nlm_apply_x01_mtx.shape)\n",
    "display(nlm_apply_x01_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1293ba41-c40d-4ed7-86d1-7e7326b70c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_samp_dwm(sm=nlm_apply_x01_mtx,\n",
    "                 vec=nlm_tfidf,\n",
    "                 n=(17,11),\n",
    "                 rs_tup=(5,1699))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fcef9f-104b-46f7-8d72-8616f60da163",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm_apply_mtx_pred_prob = m2v1_gbc.predict_proba(nlm_apply_x01_mtx)\n",
    "\n",
    "print(nlm_apply_mtx_pred_prob.shape)\n",
    "print(nlm_apply_mtx_pred_prob[:10])\n",
    "\n",
    "nlm_apply_mtx_pred = m2v1_gbc.predict(nlm_apply_x01_mtx)\n",
    "\n",
    "print(nlm_apply_mtx_pred.shape)\n",
    "print(nlm_apply_mtx_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9297995-7a7d-49ad-b968-ec96f5bb847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the maximum values along the second dimension\n",
    "max_values = np.amax(nlm_apply_mtx_pred_prob, axis=1)\n",
    "max_values_df01 = pd.DataFrame(max_values,\n",
    "                               columns=['decision_prob'])\n",
    "max_values_df01['pred'] = nlm_apply_mtx_pred\n",
    "print(max_values_df01.shape)\n",
    "display(max_values_df01.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190cc10-ee5c-4b5a-a463-29330d060d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_values_df01['decision_prob'].plot(kind=\"hist\",\n",
    "                                      density=True,\n",
    "                                      alpha=0.5,\n",
    "                                      legend=True,\n",
    "                                      figsize=(10,7),\n",
    "title='''Gradient Boost Model Probability Distribution\\n\n",
    "                                      Applied to Customer Articles''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1af562-f5a8-42c8-919c-ae68d58bee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_values_df01.groupby('pred')['decision_prob'].plot(kind=\"hist\",\n",
    "                                                      density=True,\n",
    "                                      alpha=0.5,\n",
    "                                      legend=True,\n",
    "                                      figsize=(10,7),\n",
    "                          title='''Gradient Boost Model Probability Distribution\n",
    "Prediction Confidence''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21ca79b-f9b4-4fc2-98cb-5b7712975b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_values_df02 = pd.DataFrame(nlm_apply_mtx_pred_prob.round(4),\n",
    "                               columns=['left', 'right'])\n",
    "max_values_df02['pred'] = nlm_apply_mtx_pred\n",
    "max_values_df02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf6a1f1-554a-4cf9-abf2-1d393f268c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms\n",
    "#plt.hist(max_values_df02['left'], bins=10, alpha=0.5, color='blue', label='Column 1')\n",
    "plt.hist(max_values_df02['right'], bins=10, alpha=0.5, color='red',\n",
    "         label='right')\n",
    "\n",
    "# Adding legend and title\n",
    "plt.legend()\n",
    "plt.title('Histogram of Right Prediction Probabilities')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e76026-8cc3-4f6c-9317-e61232684fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms\n",
    "#plt.hist(max_values_df02['left'], bins=10, alpha=0.5, color='blue', label='Column 1')\n",
    "plt.hist(max_values_df02['left'], bins=10, alpha=0.5, color='blue',\n",
    "         label='left')\n",
    "\n",
    "# Adding legend and title\n",
    "plt.legend()\n",
    "plt.title('Histogram of Left Prediction Probabilities')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280312c-8e11-4a77-9617-156948366dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms\n",
    "plt.hist(max_values_df02['left'], bins=10,\n",
    "         alpha=0.5, color='blue', label='left')\n",
    "plt.hist(max_values_df02['right'], bins=10,\n",
    "         alpha=0.5, color='red', label='right')\n",
    "\n",
    "# Adding legend and title\n",
    "plt.legend()\n",
    "plt.title('Histogram of Left/Right Prediction Probabilities')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
